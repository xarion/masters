% !TEX root = ../thesis.tex

\iffalse
    What is the strongest and most important statement that you can make from your observations? 
    If you met the reader at a meeting six months from now, what do you want them to remember about your paper? 
    Refer back to problem posed, and describe the conclusions that you reached from carrying out this investigation, summarize new observations, new interpretations, and new insights that have resulted from the present work.
    Include the broader implications of your results. 
    Do not repeat word for word the abstract, introduction or discussion.
\fi

In this research, we have investigated some methods to reduce the computational cost of convolutional neural networks. To do that, we experimented with some methods that could be used to define models with lower computational cost. We also experimented with some methods to reduce the computational complexity of a given model. 

To be able to experiment with pruning using larger models, we have implemented a tool to describe pruning routines. We have also implemented a tool that applies simple quantization, pruning and factorization methods to trained models. Using these tools, we have observed that these methods reduce the computational cost of sufficiently large models.

In our experiments we have observed that the models using separable convolutions with non-linearity results with a slightly better accuracy compared to models using convolution or kernel compositing convolution operations while requiring a significantly smaller number of operations. Using them, we have redefined residual blocks and designed a model that achieves similar results to ResNet-20 on CIFAR-10 classification task. Our model is two times wider, however it has fewer residual blocks, using two times fewer parameters and requiring 3 times fewer operations. However, more work needs to be done to achieve similar results using ImageNet dataset.

When developing models aimed for processing power restricted environments, we think that designing and training small models based on the requirements is a more stable alternative to compressing large networks. We have seen that wider and shallower residual networks using separable residual blocks are one way of designing such models.
