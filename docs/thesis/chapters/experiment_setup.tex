% !TEX root = ../thesis.tex
\section{Baseline Models}

We used 4 baseline models to test some of the methods we have described. These models helped us to assess the efficiency of some methods that we have covered.

\subsection{Fully Connected Summation}
We have implemented a neural network consisting of 2 inputs, $\mathbf{i} = (i_1, i_2)$, 1 fully connected layer with $n = 1000$ hidden nodes and 1 output, $o$. We have used ReLU \cite{nair2010rectified} activations on our hidden layer. For the sake of simplicity, we have defined the expected output $y$ as $y = i_1 + i_2$. We chose a simple problem so that we precisely know the most optimum neural network structure that would be able to perform this calculation. Which is the same network where the fully connected layer has one hidden node, all weights equal to $1$ and all biases equal to $0$.

We have calculated the loss using mean squared error, and optimized it using Momentum Optimizer (learning rate $0.01$ and momentum $0.3$). Using $1.000.000$ samples, we trained the network with batch size $1000$. With these parameters, we ran a training session with 10 epochs and we have observed that the loss didn't converge to 0. Therefore, the model was unable to find the correct solution with this optimizer. 

\subsection{MNIST auto-encoder}
we have implemented an auto encoder for MNIST Dataset \cite{lecun1998mnist}. MNIST contains $28 \times 28$ grayscale images of handwritten digits. The autoencoder consists of two parts. First part is the encoder. The encoder aims to reduce the dimensionality of input. The decoder aims to convert the encoded data back to it's original form.

We have defined the auto encoder with two encoder blocks followed by two decoder blocks. Each encoding block is running convolutions with kernel size $3$, strides of $2$ and $SAME$ padding. Then we are adding bias to this result, following this we are applying batch normalization \cite{ioffe2015batch} and then ReLU activation \cite{nair2010rectified}. Each decoding block is running deconvolutions with kernel size $3$ and strides of $2$. Followed by adding bias, batch normalization and ReLU activations. 

The information contained in one $28 \times 28 \times 1$ matrix is represented with $784$ units (floating points in this case). Therefore, a good auto-encoder should be capable of reducing this number when encoding. Similarly, converting the reduced matrix back to it's original form with minimal loss while decoding. The baseline auto-encoder we will compare our results is the non-encoding one given in table \ref{tab:mnist_baseline_encoder}.

In our case, our encoder blocks are reducing the matrix width and height to half. Therefore, if they output $4$ times the number of input channels, they should represent the same information losslessly. Similarly our decoder blocks are doubling the matrix width and height. Therefore if they output a quarter of the number of input channels, they should be able to decode the encoded information perfectly. In Table \ref{tab:mnist_baseline_encoder} we have defined the layer output dimensions for that baseline auto-encoder.
\begin{table}
\begin{center}
\begin{tabular}{ c | c }
 Block Name & Output Dimensions ($h \times w \times c$) \\
 \hline
 Input Image & $28 \times 28 \times 1$ \\
 Encoder 1 & $14 \times 14 \times 4$ \\  
 Encoder 2 & $7 \times 7 \times 16$ \\
 Decoder 1 & $14 \times 14 \times 4$ \\  
 Decoder 2 & $28 \times 28 \times 1$ 
\end{tabular}
\end{center}
\caption{The baseline network that could perform lossless encoding in theory.}
\label{tab:mnist_baseline_encoder}
\end{table}

To define the network to experiment on, we chose $[32, 64, 32]$ as the output channels of Encoder 1, Encoder 2 and Decoder 1 respectively.

\todoin{This definition may not be good. check it.}

\subsection{MNIST Classifier}
Our MNIST Classifier is consisting of three Convolutional Layers and one Fully Connected layer. This configuration is defined in Table \ref{tab:nxn-mnist-classifier}. Please note that the third convolution is a $7 \times 7$ convolution applied to a $7 \times 7$ input, with $VALID$ padding. Therefore, this layer is working as a fully connected layer. We keep it as a convolution operation to be able to experiment with convolution operations.

While initializing the weights, we use a truncated normal distribution with $0$ mean and $0.1$ standard deviation. We calculate the loss by the cross entropy between labels and logits. To train this network, we use RMSProp Optimizer with learning rate $10^{-4}$. We train this network for $20000$ steps with batch size $50$. To express the performance of this model, we use the accuracy score on the test dataset.
\todoin{what is the accuracy of baseline model\\
\textbf{change the standard deviation of weight distribution to 0.1, remove biases} and redo the experiments by increasing the learning rate and momentum.}
\begin{table}
\centering
\begin{tabular}{l | c | c}
Layer & Configuration & Output\\
\hline
Input Image & & $28 \times 28 \times 1$ \\
\hline
Convolution & \small $N=5, \text{strides}=1, \text{padding}=SAME$ & $28 \times 28 \times 32$ \\
Add Bias & & $28 \times 28 \times 32$ \\
ReLU & & $28 \times 28 \times 32$ \\
Max Pool & size=$ 2 \times 2$ & $14 \times 14 \times 32$ \\
\hline
Convolution & \small $N=5, \text{strides}=1, \text{padding}=SAME$ & $14 \times 14 \times 64$ \\
Add Bias & & $14 \times 14 \times 64$ \\
ReLU & & $14 \times 14 \times 64$ \\
Max Pool & size=$ 2 \times 2$ & $7 \times 7 \times 64$ \\
\hline
Convolution & \small $N=7, \text{strides}=1, \text{padding}=VALID$ & $1 \times 128$ \\
Add Bias & & $ 1 \times 128$ \\
ReLU & & $ 1 \times 128$ \\
\hline
FC Layer &  & $1 \times 10$ \\
Add Bias & & $1 \times 10$ 
\end{tabular}
\caption{Network configuration for MNIST Classifier, output of every row is applied as the input of next.}
\label{tab:nxn-mnist-classifier}
\end{table}

\subsection{CIFAR-10 Classifier}
Our CIFAR-10 baseline model is defined in Table \ref{tab:baseline-cifar10-classifier}. The input is distorted as defined in the table. 

While initializing the weights, we use a truncated normal distribution with $0$ mean and $0.1$ standard deviation. We calculate the loss by the cross entropy between labels and logits. To train this network we use Adam Optimizer with default parameters. We train this network for $N$ steps with batch size $M$.  To express the performance of this model, we use the accuracy score on the test dataset.

In this model we are using Adam Optimizer to do the training with default parameters. The loss is the cross entropy between logits and labels.
\todoin{remove bias, add batch normalization properly and redo your experiments.}
\begin{table}
\centering
\begin{tabular}{l | c | c}
Layer & Configuration & Output\\
\hline
Input Image & & $32 \times 32 \times 3$ \\
\hline
Random Crop & & $24 \times 24 \times 3$ \\
Random Flip & left to right & $24 \times 24 \times 3$ \\
Random Brightness &  $[-63, 63]$  & $24 \times 24 \times 3$ \\
Random Contrast &  $[0.2, 1.8]$  & $24 \times 24 \times 3$ \\
Normalization &  \texttt{per\_image\_standardization}  & $24 \times 24 \times 3$ \\
\hline
Convolution & \small $N=5, \text{strides}=1, \text{padding}=SAME$ & $24 \times 24 \times 64$ \\
Add Bias & & $24 \times 24 \times 64$ \\
Batch Normalization & & $24 \times 24 \times 64$ \\
ReLU & & $24 \times 24 \times 64$ \\
Max Pool & \small $N=3, \text{strides}=2, \text{padding}=SAME$ & $12 \times 12 \times 64$ \\
\hline
Convolution & \small $N=5, \text{strides}=1, \text{padding}=SAME$ & $12 \times 12 \times 64$ \\
Add Bias & & $12 \times 12 \times 64$ \\
Batch Normalization & & $12 \times 12 \times 64$ \\
ReLU & & $12 \times 12 \times 64$ \\
Max Pool & \small $N=3, \text{strides}=2, \text{padding}=SAME$ & $6 \times 6 \times 64$ \\
\hline
FC Layer &  \small $(6*6*64) \times 384 $ & $1 \times 384$ \\
Add Bias & & $ 1 \times 384$ \\
Batch Normalization & & $ 1 \times 384$ \\
ReLU & & $ 1 \times 384$ \\
\hline
FC Layer &  $384 \times 192 $ & $1 \times 192$ \\
Add Bias & & $ 1 \times 192$ \\
Batch Normalization & & $ 1 \times 192$ \\
ReLU & & $ 1 \times 192$ \\
\hline
FC Layer & $192 \times 10$  & $1 \times 10$ \\
Add Bias & & $1 \times 10$ 
\end{tabular}
\caption{Network configuration for CIFAR-10 Classifier, output of every row is applied as the input of next.}
\label{tab:baseline-cifar10-classifier}
\end{table}


\section{Pruning Experiments}
\subsection{Fully Connected Layers}
To gain more insight on activation based pruning, we ran some experiments with Fully Connected Summation model. 
\subsubsection{Vanilla Pruning}
First we have implemented the very basic idea of pruning unused activations. To do so, we defined training cycles based on the method defined in \cite{Hu:2016aa}. In each training cycle, 1) we have trained the model for some epochs, 2) we lock the weights, 3) feed the training data to the network and count the activations for each neuron in the hidden layer, 4) prune the neurons that have less than or equal to the activation threshold, 5) go back to step $1$ if some neurons were pruned, stop otherwise.

When tested with $0$ activation threshold, after the first training cycle, this method did not to prune anymore weights. In our experiments, we have pruned approximately 950 weights out of 1000. This result is promising but at the same time, it's not close enough to the result we were expecting. We delved deeper into the source of this issue.

\todoin{We should try different optimizers and make the beginning of the case about why we decided to distort weights.Tell that we have checked the gradients and seen that they were mostly in one direction (+). }

\subsubsection{Distorted Pruning}

When we inspected the gradients of weights, we have seen that most of them were in the positive direction. In our case, this trend in gradients is not helping with the understanding of which neurons are necessary, and which are not. This trend can also be understood as, the feature representation is shared among different hidden neurons. 
\todoin{talk about what does "all gradients are in the positive direction" mean for feature representation}
To prevent shared feature representation, we have decided to distort the weights using random values. This allowed some weights to become unused, therefore getting closer to the optimum result.
\todoin{the results were in a form not resembling the real solution. maybe because floating point numbers not adding up perfectly, but the result is almost the same in terms of our loss. the exact values of weights and biases are: \\
\texttt{w1: [[ 0.74285096], [ 0.64994317]]\\
b1: [ 7.80925274]\\
w2: [[-6.75151157]]\\
b2: [ 7.80925274]}\\
So since our random values are between -1 and 1, these values are actually okay.}
\todoin{talk about how you decide on the amount of distortion (currently $rand(weights.shape) * (1-var(weights))$). Talk about what changed when we introduced }
\subsubsection{Regularized Distorted Pruning}
Since the solution we found is only resembling our result under some boundaries, we have decided to add an l1 regularizer to our loss. By doing so we are aiming to push the high bias and w2 values closer to 0. But it doesn't really make any difference when used with Moment Optimizer.

\subsection{Convolutional Layers}

To verify the validity of this method, we ran experiments with MNIST auto-encoder.
\subsubsection{Activation Based Pruning}
As we did in the Fully Connected Layers, we have pruned the connections that are not being activated. In these experiments we have seen that the network has been pruned insignificantly. After applying this method, we have achieved a network consisting of $[16, 64, 22]$ output channels for blocks Encoder 1, Encoder 2 and Decoder 1 respectively.

\subsubsection{Applying Distirtions}
\todoin{test if this actually changes anything.}
\todoin{we can also check activation probabilities and make a decisions based on this data}

\subsubsection{Applying Regularizers}
\todoin{explain why you chose this regularizer, tell the effect of using it}

\subsubsection{Pruning Outliers}
\todoin{explain why you decided to prune the "outliers" from activations, how you decide on what are outliers $(mean - 2*std)$ and how this effects the final solution}

\subsubsection{Regularized and Distorted Outilier Pruning}
\todoin{explain your results when you combined these methods. }

\section{Efficient Operations}
\subsection{1-D Convolutions}
