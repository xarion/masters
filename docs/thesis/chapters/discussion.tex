% !TEX root = ../thesis.tex

\iffalse
 Start with a few sentences that summarize the most important results. The discussion section should be a brief essay in itself, answering the following questions and caveats: 

    What are the major patterns in the observations? (Refer to spatial and temporal variations.)
    What are the relationships, trends and generalizations among the results?
    What are the exceptions to these patterns or generalizations?
    What are the likely causes (mechanisms) underlying these patterns resulting predictions?
    Is there agreement or disagreement with previous work?
    Interpret results in terms of background laid out in the introduction - what is the relationship of the present results to the original question?
    What is the implication of the present results for other unanswered questions in earth sciences, ecology, environmental policy, etc....?
    Multiple hypotheses: There are usually several possible explanations for results. Be careful to consider all of these rather than simply pushing your favorite one. If you can eliminate all but one, that is great, but often that is not possible with the data in hand. In that case you should give even treatment to the remaining possibilities, and try to indicate ways in which future work may lead to their discrimination.
    Avoid bandwagons: A special case of the above. Avoid jumping a currently fashionable point of view unless your results really do strongly support them. 
    What are the things we now know or understand that we did not know or understand before the present work?
    Include the evidence or line of reasoning supporting each interpretation.
    What is the significance of the present results: why should we care? 

This section should be rich in references to similar work and background needed to interpret results. However, interpretation/discussion section(s) are often too long and verbose. Is there material that does not contribute to one of the elements listed above? If so, this may be material that you will want to consider deleting or moving. Break up the section into logical segments by using subheads. 
\fi

Here we will try to criticize our decisions in model and method selection which are increasing the questionability of our results. We will also talk about the problems we have faced during this research. 

We think that some of our design decisions regarding the pruning experiments were wrong or misleading. First, to select the best nodes to prune, we tried to make use of some pruning criteria. However, in our experiments, we have seen that randomly pruning nodes and leaving out some nodes also produced similar results. We think that this is the result of using high learning rates in the beginning of every training cycle. To fix that, we could use smaller learning rates in every training cycle, but this would make our model converge to a non-optimal structure. Second, we started with very large initial models. We believe that this may have left a false impression that pruning methods will reduce the model complexity by 1000 times. We think that this also is the case for some recent work on pruning. Given a sufficiently large model and a sufficiently small problem, creating a desired pruning rate is possible. We think that pruned models should also be compared with other small models defined for the task. Third, our pruning criteria selection was based on very simple methods. However, more complicated methods, such as aforementioned \textit{frequency sensitive hashing} or some gradient based pruning criteria may have worked better on small models. 

Also when we were applying the pruning techniques on small models, we may have made some bad decisions. First, while pruning residual networks, we have set a very strict rule, that grouped separable residual blocks and pruned their output features together. This is a very strong assumption. We could also define a rule to prune residual blocks separately and place zeros on pruned indices before residual connections with previous layers. However, we decided to stay away from this complicated method. Second, the autoencoder model that we have experimented with is not a classification model. However, we have applied the best practices that worked for this model to prune a classification model. Instead of experimenting with an autoencoder, we should have worked with a classifier. Third, since residual blocks have a one to one relationship between their output channels it may have been possible to prune residual blocks in some cases. However, we didn't have time to try such a method.

In Section \ref{sec:conv_alternatives}, we have defined a neural network to compare alternative convolution operations. Before our experiments, we have tried to find the best settings for some parameters, such as learning rate, regularization constant and optimizer. We think that using the same settings may have influenced our results. Especially because the number of parameters reduce considerably when we use alternative operations. This may have worked in favor of alternative methods if we made a bad choices in selecting these parameters. However, one can also argue that same parameters should be used in such a comparison. Also, we have used large ($5 \times 5$) kernels for these experiments. We are not sure if these results can be translated to $3 \times 3$ kernels. However, to be able to compare kernel composing convolutions with others, we felt an urge to use larger kernels. We could have separated separable convolution and kernel composing convolution experiments and compared them separately with convolution operations using relatively large kernels. 

In Section~\ref{sec:result_models}, we have compared our CIFAR-10 model with ResNet-20 (\cite{He:2015aa}). However there are small differences between how these models were trained. First ResNet-20 is trained using a different data preprocessing and augmentation technique. Second, ResNet-20 (\cite{He:2015aa}) does not employ full pre-activation residual connections. ResNet-20 may have performed better with these techniques.

Compared to MNIST and CIFAR-10, ImageNet is a very large dataset. Using CIFAR-10, we were able to search the parameter space for the smallest model that perform well. However, since training a model for ImageNet takes about a week with the available equipment, we were unable find the best model. Also, we think that our way of aggressively reducing image dimensions was very experimental, and probably unstable. More experiments are required to prove this configuration valid or invalid.

\subsubsection{Using Tensorflow}
We have been using latest versions of Tensorflow. It comes with some advantages, such as:
\begin{itemize}
\item We do not need to implement lower level operations (such as convolutions). It gives us the opportunity to focus on higher level implementations, such as pruning, or factorization. 
\item Most of the operations are highly optimized for many platforms and devices. If we were to implement a model in C++, we'd have to implement it twice, one for training in GPU and another for running in the mobile device. 
\item Tensorflow provides the necessary tools to deploy models on mobile devices.
\end{itemize}

And it comes with some disadvantages, such as:

\begin{itemize}
\item When we started our work, Tensorflow was in version 0.10. By the date we write this, it is on 1.2. There have been 4 major releases that we had to modify our codebase for.
\item Not all operations are properly implemented. For example, before version 1.2, Tensorflow implementation of separable convolutions were not very well optimized. They were as fast as convolution operations. Before that we could only hope that they would optimize their implementation.
\item It is difficult to implement operations (e.g. ef operator \cite{afrasiyabi2017energy}) or play around with existing ones. The documentation describing C++ internals and build procedures (as of Tensorflow 1.2) are not good enough. 
\item Tensorflow does not provide tools to implement low-bit variables (e.g. a 2-bit integer). So it is not possible to implement some methods that make use of variable width decimals. This limitation makes some methods impossible to use or useless. For example it is not possible to use methods that represent weights using variable width decimals. Also, storing low bit weight indices in combination with a small global weight array to reduce the model size is useless. Since we can not use low bit integers to represent these indices, our model size does not shrink at all.
\end{itemize}
