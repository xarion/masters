% !TEX root = ../thesis.tex

\iffalse
 Start with a few sentences that summarize the most important results. The discussion section should be a brief essay in itself, answering the following questions and caveats: 

    What are the major patterns in the observations? (Refer to spatial and temporal variations.)
    What are the relationships, trends and generalizations among the results?
    What are the exceptions to these patterns or generalizations?
    What are the likely causes (mechanisms) underlying these patterns resulting predictions?
    Is there agreement or disagreement with previous work?
    Interpret results in terms of background laid out in the introduction - what is the relationship of the present results to the original question?
    What is the implication of the present results for other unanswered questions in earth sciences, ecology, environmental policy, etc....?
    Multiple hypotheses: There are usually several possible explanations for results. Be careful to consider all of these rather than simply pushing your favorite one. If you can eliminate all but one, that is great, but often that is not possible with the data in hand. In that case you should give even treatment to the remaining possibilities, and try to indicate ways in which future work may lead to their discrimination.
    Avoid bandwagons: A special case of the above. Avoid jumping a currently fashionable point of view unless your results really do strongly support them. 
    What are the things we now know or understand that we did not know or understand before the present work?
    Include the evidence or line of reasoning supporting each interpretation.
    What is the significance of the present results: why should we care? 

This section should be rich in references to similar work and background needed to interpret results. However, interpretation/discussion section(s) are often too long and verbose. Is there material that does not contribute to one of the elements listed above? If so, this may be material that you will want to consider deleting or moving. Break up the section into logical segments by using subheads. 
\fi

Here we will try to reason about our results, criticize our decisions in model and method selection. We will also talk about the problems we have faced during this research. 

We think that some of our design decisions regarding the pruning experiments were wrong or misleading. First, to select the best nodes to prune, we tried to find the best pruning criterion. However, in our experiments, we have seen that randomly pruning nodes and leaving out some nodes also produced similar results. We think that this is the result of using high learning rates in the beginning of every training cycle. To fix that, we could use smaller learning rates in every training cycle, but this would make our model converge to a non-optimal structure. Second, we started with very large initial models. We believe that this may have left a false impression that our pruning methods will reduce the number of nodes by 1000 times for any given model. We think that this is also the case for some recent work on pruning. In our research, we have seen that the ability to greatly reduce model complexity using pruning implies that the model is heavily over-parameterized for the problem. Finally, our pruning criteria selection was based on very simple methods. However, more complicated methods, such as aforementioned \textit{frequency sensitive hashing} or some gradient based pruning criteria may have worked better on small models.

In Section \ref{sec:conv_alternatives}, we have defined a neural network to compare alternative convolution operations and we have seen that nonlinear separable convolutions outperform the baseline convolution operations and kernel composing convolution operations. When we were designing our experiments, we have tried to find the best values for some parameters, such as learning rate and regularization constant. We think that using the same setting may have influenced our results. This may have worked in favor of alternative methods especially because they have smaller number of parameters. However, one can also argue that the same setting should be used in such a comparison. Moreover, we have used large kernels ($5 \times 5$) for these experiments so that the number of parameters would reduce greatly when an alternative operation is used. However, we do not know if these results can be translated to $3 \times 3$ kernels.

In Section~\ref{sec:result_models}, we have compared our CIFAR-10 model with ResNet-20 (\cite{He:2015aa}). It should be noted that there are small differences between how these models were trained. First ResNet-20 is trained using a different data preprocessing and data augmentation technique. Second, ResNet-20 (\cite{He:2015aa}) does not employ full pre-activation residual connections. ResNet-20 may have performed better with these techniques.

In Section~\ref{sec:small_models} when we have defined our ImageNet model, we started with the assumption that recreating ResNet-34 with separable residual blocks and aggressive dimensionality reduction would perform well. However, compared to MNIST and CIFAR-10, ImageNet is a very large dataset. Using CIFAR-10, we were able to search the parameter space for the smallest model by validating our assumptions and found an optimum model. However, since training a model for ImageNet took about a week with the available equipment, we were unable to do the same thing for our ImageNet model. Also, we think that our proposal to aggressively reduce image dimensions effected our results negatively. We think that this method requires more experimentation and fine tuning.

In Section~\ref{sec:pruning_small_models}, we stated that we were unable to find a pruning criterion that would provide an essential performance improvement while preserving the accuracy. We think that this is because our model was well balanced in terms of complexity and accuracy. However, we may have made some bad decisions in search for a criterion. First, while pruning residual networks, we have set a very strict rule that grouped separable residual blocks and pruned their output features together. This is a very strong assumption. We could also define a rule to prune residual blocks separately and place zeros on pruned indices before residual connections with previous layers. However, we decided to stay away from this complicated method. Second, the autoencoder model that we have experimented with is not a classification model. However, we have applied the best practices that worked for this model to prune a classification model. Instead of experimenting with an autoencoder, we should have worked with a classifier. Third, since residual blocks have a one to one relationship between their output channels it may have been possible to prune some of the residual blocks as a whole in some cases. However, we didn't have time to try such a method. 

Similar to the pruning experiments, in Section~\ref{sec:approximating_small_models}, we were unable to find a good approximation for our small model. We think that this also supports the theory that our model is well balanced in terms of complexity and accuracy.

In Section~\ref{sec:quantizing_small_models} we stated that quantization slowed down the inference speed by half. We think that this is caused by the suboptimal operation implementations. 

\subsubsection{Using Tensorflow}
We have been using latest versions of Tensorflow. It comes with some advantages, such as:
\begin{itemize}
\item We do not need to implement lower level operations (such as convolutions). It gives us the opportunity to focus on higher level implementations, such as pruning, or factorization. 
\item Most of the operations are highly optimized for many platforms and devices. If we were to implement a model in C++, we'd have to spend considerable effort in optimizing it for efficient use of memory and processor. In such a case comparing various techniques and models would take considerable time.
\end{itemize}

And it comes with some disadvantages, such as:

\begin{itemize}
\item When we started our work, Tensorflow was in version 0.10. By the date we write this, it is on 1.2. There have been 4 major releases that we had to modify our codebase for.
\item Not all operations are properly implemented. For example, before version 1.2, Tensorflow implementation of separable convolutions were not very well optimized. They were as fast as convolution operations. Before that we could only hope that they would optimize their implementation.
\item It is difficult to implement new operations and modify the existing ones because the C++ internals and build procedures (as of Tensorflow 1.2) are not well documented. 
\item Tensorflow does not provide tools to implement low-bit variables (e.g. a 2-bit integer). So it is not possible to implement some methods that make use of variable width decimals. This limitation makes some methods impossible to use or useless. For example it is not possible to use methods that represent weights using variable width decimals. Also, storing low bit weight indices in combination with a small global weight array to reduce the model size is useless. Since we cannot use low bit integers to represent these indices, our model size does not shrink at all.
\end{itemize}
