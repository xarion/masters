% !TEX root = ../thesis.tex
\iffalse
Introduction
You can't write a good introduction until you know what the body of the paper says. Consider writing the introductory section(s) after you have completed the rest of the paper, rather than before.

Be sure to include a hook at the beginning of the introduction. This is a statement of something sufficiently interesting to motivate your reader to read the rest of the paper, it is an important/interesting scientific problem that your paper either solves or addresses. You should draw the reader in and make them want to read the rest of the paper.

The next paragraphs in the introduction should cite previous research in this area. It should cite those who had the idea or ideas first, and should also cite those who have done the most recent and relevant work. You should then go on to explain why more work was necessary (your work, of course.)
 
What else belongs in the introductory section(s) of your paper? 

    A statement of the goal of the paper: why the study was undertaken, or why the paper was written. Do not repeat the abstract. 
    Sufficient background information to allow the reader to understand the context and significance of the question you are trying to address. 
    Proper acknowledgement of the previous work on which you are building. Sufficient references such that a reader could, by going to the library, achieve a sophisticated understanding of the context and significance of the question.
    The introduction should be focused on the thesis question(s).  All cited work should be directly relevent to the goals of the thesis.  This is not a place to summarize everything you have ever read on a subject.
    Explain the scope of your work, what will and will not be included. 
    A verbal "road map" or verbal "table of contents" guiding the reader to what lies ahead. 
    Is it obvious where introductory material ("old stuff") ends and your contribution ("new stuff") begins? 

Remember that this is not a review paper. We are looking for original work and interpretation/analysis by you. Break up the introduction section into logical segments by using subheads. 
\fi

The state of the art in Image Processing has changed when graphics processing units (GPU) were used to train neural networks. GPUs contain many cores, they have very large data bandwidth and they are optimized for efficient matrix operations. In 2012, \cite{krizhevsky2012imagenet} won the ImageNet Large Scale Visual Recognition Competition (ILSVRC) classification task (\cite{deng2012image}). They used two GPUs to train an 8 layer convolutional neural network (CNN). Their model has improved the previous (top-5) classification accuracy record from $\sim 74\%$ to $\sim 84\%$. This caused a big trend shift in Computer Vision. 

As the years pass, GPUs got more and more powerful. In 2012, \cite{krizhevsky2012imagenet} used GPUs that had 3 GB memory. Today there are GPUs with up to 12 GB memory. The number of floating point operations per second (FLOPs) has also increased from $2.5$ tera FLOPs (TFLOPs) to $12$ TFLOPs. This gradual but steep change has allowed the use of more layers and more parameters. For example, \cite{Simonyan:2014aa} introduced a model called VGGNet. Their model used up to 19 layers and shown that adding more layers affects the accuracy. \cite{He:2015aa} introduced a new method called residual connections, that allowed the use of up to 200 layers. Building up on such models, in 2016 ILSVRC winning (top-5) classification accuracy is increased to $\sim 97\%$. 

In contrast, \cite{Szegedy:2014aa} have shown that having a good harmony within the network worked better than having more parameters. It has been supported by \cite{Canziani:2016aa}. They have shown the relation between number of parameters of a model and its top-1 classification accuracy in ILSVRC dataset. According to their report, 48 layer Inception-v3 (\cite{Szegedy_2016_CVPR}) provides better top-1 classification accuracy than 152 layer ResNet (\cite{He:2015aa}). They also show that Inception-v3 requires fewer number of floating point operations to compute results. Therefore, revealing that of providing more layers and parameters would not yield better results. 

ILSVRC is one of the most famous competitions in Image Processing. Every year, the winners of this competition are a driving the research on the field. But this competition is not considering the competitive value of limiting number of operations. If we look at the models of 2016 competitors, we see that they use ensembles of models\footnote{\url{http://image-net.org/challenges/LSVRC/2016/results\#team}}. These ensembles are far from being usable in real life because they require a great amount of operations per inference. Not mentioning the number of operations from the result is misleading for the AI community and the public. It creates an unreal expectation that these models are applicable in real life. In this thesis, we want to come up with a state of the art solution that requires a low number of floating point operations per inference. Therefore, bridging the gap between expectations and reality.

\section{Background}
In this chapter, we will try to describe neural networks briefly. To keep things simple, we are concerned with feed forward neural networks for now. We will provide some terminology and give some examples. 

\subsection{Neural Networks}
Neural networks are \textit{weighted graphs}. They consist of an ordered set of \textit{layers}, where every layer is a set of \textit{nodes}. The first layer of the neural network is called the \textit{input layer}, and the last one is called the \textit{output layer}. The layers in between are called \textit{hidden layers}. In our case, nodes belonging to one layer are connected to the nodes in the following and/or the previous layers. These connections are weighted edges, and they are mostly called as \textit{weights}. 

Given an input, neural networks nodes have \textit{outputs}, which are real numbers. The output of a node is calculated by applying a function ($\psi$) the outputs of the nodes belonging to previous layers . Preceding that, the output of the input layer is calculated using the input data (see Eq. \ref{eq:output_of_layers}).  By calculating the layer outputs consecutively we calculate the output of the output layer. This process is called \textit{inference}. We use the following notations to denote the concepts that we just explained.
\begin{equation}
\label{eq:variable_definitions}
\begin{split}
l_k & \text{: a column vector of nodes for layer $k$}\\
m_k & \text{: the number of nodes in $l_k$}\\
l_{k,i}  & \text{: node $i$ in $l_k$}\\
o_{k}  & \text{: the output vector representing the outputs of nodes in $l_{k}$}\\
o_{k,i}  & \text{: the output of $l_{k,i}$}\\
\mathbf{w}^{(k)}  & \text{: weight matrix connecting nodes in $l_{k-1}$ to nodes in $l_{k}$} \\
w^{(k)}_{i,j}  & \text{: the weight connecting nodes $l_{(k-1),i}$ and $l_{k,j}$} \\
\mathbf{b}^{(k)}  & \text{: the bias term for $l_{k}$} \\
\psi_k & \text{: function to determine $o_k$ given $o_{k-1}$}\\
\sigma & \text{: activation functions} \\
\mathbf{x} & \text{: all inputs of the dataset, consisting of $N$ data points} \\
\mathbf{y} & \text{: all outputs of the dataset} \\
\mathbf{\hat y} & \text{: approximation of the output}  \\
x_n & \text{: $n$th input data ($0 < n \leq N$)} \\
y_n & \text{: $n$th output data ($0 < n \leq N$)} \\
\hat y_n & \text{: approximation of $y_n$ given $x_n$ ($0 < n \leq N$)}\\
\text{FC} & \text{: stands for Fully Connected (e.g. $\psi^{(FC)}$)}
\end{split}
\end{equation}
Therefore the structure of a neural network is determined by the number of layers and the functions that determine the outputs of layers.
\begin{equation}
\label{eq:output_of_layers}
    o_k = 
\begin{cases}
    \psi(o_{k-1}), &\text{if } k\geq 1\\
    \mathbf{x},& k = 0\\
\end{cases}
\end{equation}

\subsection{Fully Connected Layers}
As the name suggests, for two consecutive layers to be \textit{fully connected}, all nodes in the previous layer must be connected to all nodes in the following layer. 

Let's assume two consecutive layers, $l_{k-1}$ and $l_{k}$, with shapes $m_{k-1} \times 1$ and $m_k \times 1$, respectively. For these layers to be fully connected, the weight matrix $\mathbf{w}^{(k)}$, should be of shape $m_{k-1} \times m_{k}$. Most fully connected layers also include a bias term ($m$) for every node $l_k$. In fully connected layers, $o_k$ would simply be calculated using layer function $\psi^{(FC)}$.
$$ \psi^{(FC)}_k(o_{k-1}) = o_{k-1}^T\mathbf{w}^{(k)} + \mathbf{b}^{(k)}$$
Therefore the computational complexity of $\psi^{(FC)}$ would become
$$\mathcal{O}(\psi^{(FC)}_k) = m_{k-1}m_{k}$$

\subsection{Activation Function and Nonlinearity}
By stacking fully connected layers, we can increase the depth of a neural network. By doing so we want to increase approximation quality of the neural network. However, the $\psi^{(FC)}$ we have defined is a linear function. Therefore if we stack multiple fully connected layers we would end up with a linear model. 

To achieve non-linearity, we apply \textit{activation functions} to the results of $\psi$. There are many activation functions (such as $tanh$ or $sigmoid$) but one very commonly used activation function is $ReLU$.  
\begin{equation}
\label{eq:relu_definition}
    ReLU(x) = 
\begin{cases}
    x, & \text{if }x \geq 0\\
    0 &  \text{otherwise }\\
\end{cases}
\end{equation}
Therefore we will redefine the fully connected $\psi^{(FC)}$ as;

$$ \psi^{(FC)}_k(o) = \sigma(o^T\mathbf{w}^{(k)} + \mathbf{b}^{(k)})$$

$\psi^{(FC)}$ is one of the most basic building blocks of any Neural Network. Stacking $K$ of them after the input, we can try to approximate an output given an input. To do that we will calculate the outputs of every layer, starting from the input. 
\begin{equation*}
\begin{split}
o_0 &= x_n\\
o_1 &= \psi_1^{(FC)}(o_0)\\ 
o_2 &= \psi_2^{(FC)}(o_1)\\
...&\\
o_K &= \psi_1^{(FC)}(o_{K-1})\\
\hat y &= o_K
\end{split}
\end{equation*}

\subsection{Loss}
To represent the quality of an approximation, we are going to use a loss (or cost) function. A good example would be the loss of a salesman. Assuming a customer who would pay at most \$10 for a given product, if the salesman sells this product for \$4, the salesman would face a loss of \$6 from his potential profit. Or if the salesman tries to sell this product for \$14, the customer will not purchase it and he will face a loss of \$10. In this example, the salesman would want to minimize the loss to earn as much as possible. 

However, most of the problems we will see are not that simple. When we face multi-dimensional data with multiple examples, we need a bit more complicated loss function. A common loss function is Root Mean Square Error (RMSE). Given some approximations ($\mathbf{\hat y}$) and corresponding outputs ($\mathbf{y}$), RMSE can be calculated as,
\begin{equation*}
RMSE(\mathbf{\hat y}, \mathbf{y}) = \sqrt{\frac{\sum^N_{n=1} (\hat y_n - y_n)^2 }{N}}
\end{equation*}
There are two common properties of loss functions. First, loss is never negative. Second, if we compare two approximations, the one with a smaller loss is better.

If all our approximations are exactly the same as the output ($  \mathbf{\hat y} =  \mathbf{y} $), the total loss would be 0. 

\subsection{Stochastic Gradient Descent}
\todoin{Check with Bishop first, then write this section.}
To provide better approximations, we will try to optimize our parameters (weight and bias). One common way to optimize these parameters is to use Stochastic Gradient Descent (SGD). SGD starts with some initial parameter values. It calculates the loss for a subset of data points, calculates the partial derivatives of the loss function for every parameter, and trying to minimize 

Therefore, we are interested in the optimal values of $\mathbf{W}$ that achieve that. 
One common way to optimize neural networks weights is to use Gradient Descent. Gradient descent works with iterations. In each iteration, we calculate the partial derivative of the loss function for weights. To do that, first we need to write the loss function in terms of weight variables.
\begin{equation*}
\begin{split}
 &= \sqrt{\frac{\sum^n_{t=1} (\hat y_t - y_t)^2 }{n}}\\
 &= \sqrt{\frac{\sum^n_{t=1} (\hat y_t - y_t)^2 }{n}}
\end{split}
\end{equation*}


\subsection{Convolutional Layer}

Assume that our input data points are defined as images. Image data is mostly 3 dimensional. We will represent the dimensions of an image as, $W \times H \times C$ for width, height and channels respectively. 

The convolutional layer creates $K \times K \times C$ patches from a given input. Then it multiplies this patch with a weight matrix of size $K \times K \times C \times O$ where $O$ is the number of output channels. Let's assume that we have a patch from a layer $k$ at point $(I, J)$; 

\todoin{$p(o_k, I,J, K)$ could also be something like $p^{(I,K)}_{k,(i.j)}$ also maybe check bishop or something to see a better definition. maybe include padding and strides as well. tell what a kernel is, define output/input channels better in the definition of a patch.}
\begin{equation}
\label{eq:convolution_patches}
\begin{split}
    p^{(I,K)}_{k,(i.j)} = &
\begin{cases}
    o_{k,(I-K/2+i, J-K/2+j)}, & \text{if } (0,0) < (I-K/2+i, J-K/2+j) \leq (W,H)\\
    0 &  \text{ otherwise }\\
\end{cases}\\ 
&\text{where } 0 \leq i < I \text{ and } 0 \leq j < J
\end{split}
\end{equation}

$$ o_{k, (I,J)} = \psi_k^{(Conv)}() =  p\mathbf{w_k}$$
$$ o_{k+1,(i,j)} = \psi^{(Conv)}_k(o_{k-1}) $$

\section{Datasets}


\subsubsection{MNIST}

\section{Dependencies}
\subsection{Tensorflow}
In our research, we will strictly use Tensorflow \cite{abadi2016tensorflow}. \todoin{What is tensorflow, why we chose it, what are the advantages of using it, what are the limitations that come with it}
\subsection{CIFAR-10}
\subsection{MNIST}




























