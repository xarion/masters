Recent state of the art Deep Learning models are surpassing previous methods. Fields such as; computer vision, automatic speech recognition, natural language processing, speech recognition, and bioinformatics make use of these models. They use deep models consisting of many layers (e.g. 152 layers in  \cite{He:2015aa}), many parameters in each layer, and as a result, a lot of Floating Point Operations to run Inference (e.g. $11.3 \times 10^9$ in  \cite{He:2015aa}).
In contrast, mobile devices have limited processing power and memory. Also the best practice is to provide a fluent user experience with low response time. Thus, we should change these models to provide a good user experience.
There is research on methods to define optimized models or optimize a given model. These methods consist; pruning unimportant parameters, using less bits to represent parameters, or using less parameters by using more optimized structures. 

In this research we are going run experiments to answer;
\begin{enumerate}
\item    Which models are running slow in Mobile Devices?
\item    Why these models are running slow?
\item    Which methods can we use to optimize these models?
\item    What is the trade off of using these methods?
\item    Why an optimization technique is working or not on a model?
\item    Can we define a more optimized model for the same task?
\item    How can we combine different optimization techniques?
\item    Are these optimized models efficient enough to run in Mobile Devices? 
\end{enumerate}


\todo[inline]{things to be explained: FC layer, activation, bias, training dataset, (neuron or node)}
\section{Recent Studies}
Artificial Neural Networks (ANN) have several parameters such as number of hidden layers, number of neurons in a layer, or the structure of a layer. Until now, we have seen different combinations for these parameters. For example, \cite{Simonyan:2014aa} introduces a model called VGGNet. VGGNet introduces more layers (16 to 19) than the previous models. They show how this parameter effects the accuracy. \cite{He:2015aa} introduces the residual connections. This new connection between layers is capable of stacking more layers than before. Training up to 152 layers, they show superior accuracy. \cite{Zagoruyko:2016aa} compares having higher number of neurons in each layer to having more layers. Each combination resulting in a unique model with a different accuracy level. In contrast to all these, \cite{Szegedy:2014aa} suggests something different. Having a good harmony within the network works better than having more parameters. Supporting that, \cite{Canziani:2016aa} does a detailed comparison of different models. They show that, increasing the number of hidden layers or the number of neurons in a layer does not necessarily increase the accuracy. 

Following these, we think that, some models are over-parameterized. Meaning they contain parameters that they are not making use of. Therefore, they are making unnecessary computations with them.


