% !TEX root = ../thesis.tex
\iffalse
Introduction
You can't write a good introduction until you know what the body of the paper says. Consider writing the introductory section(s) after you have completed the rest of the paper, rather than before.

Be sure to include a hook at the beginning of the introduction. This is a statement of something sufficiently interesting to motivate your reader to read the rest of the paper, it is an important/interesting scientific problem that your paper either solves or addresses. You should draw the reader in and make them want to read the rest of the paper.

The next paragraphs in the introduction should cite previous research in this area. It should cite those who had the idea or ideas first, and should also cite those who have done the most recent and relevant work. You should then go on to explain why more work was necessary (your work, of course.)
 
What else belongs in the introductory section(s) of your paper? 

    A statement of the goal of the paper: why the study was undertaken, or why the paper was written. Do not repeat the abstract. 
    Sufficient background information to allow the reader to understand the context and significance of the question you are trying to address. 
    Proper acknowledgement of the previous work on which you are building. Sufficient references such that a reader could, by going to the library, achieve a sophisticated understanding of the context and significance of the question.
    The introduction should be focused on the thesis question(s).  All cited work should be directly relevent to the goals of the thesis.  This is not a place to summarize everything you have ever read on a subject.
    Explain the scope of your work, what will and will not be included. 
    A verbal "road map" or verbal "table of contents" guiding the reader to what lies ahead. 
    Is it obvious where introductory material ("old stuff") ends and your contribution ("new stuff") begins? 

Remember that this is not a review paper. We are looking for original work and interpretation/analysis by you. Break up the introduction section into logical segments by using subheads. 
\fi

The state of the art in Image Processing has changed when graphics processing units (GPU) were used to train Neural Networks. GPU's contain many cores, they have very large data bandwidth and they are optimized for efficient matrix operations. In 2012, \cite{krizhevsky2012imagenet} won the ImageNet Large Scale Visual Recognition Competition (ILSVRC) classification task (\cite{deng2012image}). They used two GPUs to train an 8 layer Convolutional Neural Network (CNN). Their model has improved the previous (top-5) classification accuracy record from $\sim 74\%$ to $\sim 84\%$. This caused a big trend shift in Computer Vision. 

As the years pass, GPU's got more and more powerful. In 2012, \cite{krizhevsky2012imagenet} used GPU's that had 3 GB memory. Today there are GPU's with up to 12 GB memory. The number of floating point operations per second (FLOPs) has also increased from $2.5$ tera FLOPs (TFLOPs) to $12$ TFLOPs. This gradual but steep change has allowed the use of more layers and more parameters. For example, \cite{Simonyan:2014aa} introduced a model called VGGNet. Their model used up to 19 layers and shown that adding more layers effects the accuracy. \cite{He:2015aa} introduced a new method (residual connections) that allowed the use of more layers (up to 200). Building up on such models, in 2016 ILSVRC winning (top-5) classification accuracy is increased to $\sim 97\%$. 

In contrast, \cite{Szegedy:2014aa} have shown that having a good harmony within the network worked better than having more parameters. Supporting that \cite{Canziani:2016aa} has shown the relation between number of parameters of a model and its top-1 classification accuracy in ILSVRC dataset (ImageNet). According to their report, 48 layer Inception-v3 (introduced in \cite{Szegedy_2016_CVPR}) provides better top-1 classification accuracy (ImageNet) than 152 layer ResNet (introduced in \cite{He:2015aa}). They also show that Inception-v3 is requiring fewer number of FLOPs to compute results. Therefore revealing the illusion of providing more layers and parameters would yield with better results. 

ILSVRC is one of the most famous and driving competition in Image Processing. But this competition is not considering the competitive value of limiting number of operations. If we look at the methods 2016 of competitors, we see that they use ensembles of models \footnote{\url{http://image-net.org/challenges/LSVRC/2016/results\#team}}. These ensembles are far from being usable in real life because they require a great amount of operations per inference. Hiding the number of operations from the result is misleading for the AI community and the public. It creates an unreal expectation to apply these models in real life. In this study, we are going to try to come up with a state of the art solution that requires low computing power. So low that it can run on a smartphone without trouble. Therefore, bridging the gap between expectations and reality.
\section{Background}
\subsection{Datasets}
\subsubsection{MNIST}
\todoin{things to be explained: FC layer, Convolution Operation, what do we mean when we say neuron/node, activation, bias, training dataset, different paddings, SVD}
\todoin{Luc says: Like add a mathematical formulation of deep networks, how you go from NN to DNN to CNN and RNN. What are the pros and cons of these networks? Also, here I would explain all concepts that arise later (for example: what is a conv kernel, SAME padding, etc)}

\section{Dependencies}
\subsection{Tensorflow}
In our research, we will strictly use Tensorflow \cite{abadi2016tensorflow}. \todoin{What is tensorflow, why we chose it, what are the advantages of using it, what are the limitations that come with it}
\subsection{CIFAR-10}
\subsection{MNIST}