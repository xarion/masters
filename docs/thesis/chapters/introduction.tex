% !TEX root = ../thesis.tex
\iffalse
Introduction
You can't write a good introduction until you know what the body of the paper says. Consider writing the introductory section(s) after you have completed the rest of the paper, rather than before.

Be sure to include a hook at the beginning of the introduction. This is a statement of something sufficiently interesting to motivate your reader to read the rest of the paper, it is an important/interesting scientific problem that your paper either solves or addresses. You should draw the reader in and make them want to read the rest of the paper.

The next paragraphs in the introduction should cite previous research in this area. It should cite those who had the idea or ideas first, and should also cite those who have done the most recent and relevant work. You should then go on to explain why more work was necessary (your work, of course.)
 
What else belongs in the introductory section(s) of your paper? 

    A statement of the goal of the paper: why the study was undertaken, or why the paper was written. Do not repeat the abstract. 
    Sufficient background information to allow the reader to understand the context and significance of the question you are trying to address. 
    Proper acknowledgement of the previous work on which you are building. Sufficient references such that a reader could, by going to the library, achieve a sophisticated understanding of the context and significance of the question.
    The introduction should be focused on the thesis question(s).  All cited work should be directly relevent to the goals of the thesis.  This is not a place to summarize everything you have ever read on a subject.
    Explain the scope of your work, what will and will not be included. 
    A verbal "road map" or verbal "table of contents" guiding the reader to what lies ahead. 
    Is it obvious where introductory material ("old stuff") ends and your contribution ("new stuff") begins? 

Remember that this is not a review paper. We are looking for original work and interpretation/analysis by you. Break up the introduction section into logical segments by using subheads. 
\fi

The state of the art in Image Processing has changed when graphics processing units (GPU) were used to train neural networks. GPUs contain many cores, they have very large data bandwidth and they are optimized for efficient matrix operations. In 2012, \cite{krizhevsky2012imagenet} won the ImageNet Large Scale Visual Recognition Competition (ILSVRC) classification task (\cite{deng2012image}). They used two GPUs to train an 8 layer convolutional neural network (CNN). Their model has improved the previous (top-5) classification accuracy record from $\sim 74\%$ to $\sim 84\%$. This caused a big trend shift in Computer Vision. 

As the years pass, GPUs got more and more powerful. In 2012, \cite{krizhevsky2012imagenet} used GPUs that had 3 GB memory. Today there are GPUs with up to 12 GB memory. The number of floating point operations per second (FLOPs) has also increased from $2.5$ tera FLOPs (TFLOPs) to $12$ TFLOPs. This gradual but steep change has allowed the use of more layers and more parameters. For example, \cite{Simonyan:2014aa} introduced a model called VGGNet. Their model used up to 19 layers and shown that adding more layers affects the accuracy. \cite{He:2015aa} introduced a new method called residual connections, that allowed the use of up to 200 layers. Building up on such models, in 2016 ILSVRC winning (top-5) classification accuracy is increased to $\sim 97\%$. 

In contrast, \cite{Szegedy:2014aa} have shown that having a good harmony within the network worked better than having more parameters. It has been supported by \cite{Canziani:2016aa}. They have shown the relation between number of parameters of a model and its top-1 classification accuracy in ILSVRC dataset. According to their report, 48 layer Inception-v3 (\cite{Szegedy_2016_CVPR}) provides better top-1 classification accuracy than 152 layer ResNet (\cite{He:2015aa}). They also show that Inception-v3 requires fewer number of floating point operations to compute results. Therefore, revealing that of providing more layers and parameters would not yield better results. 

ILSVRC is one of the most famous competitions in Image Processing. Every year, the winners of this competition are a driving the research on the field. But this competition is not considering the competitive value of limiting number of operations. If we look at the models of 2016 competitors, we see that they use ensembles of models\footnote{\url{http://image-net.org/challenges/LSVRC/2016/results\#team}}. These ensembles are far from being usable in real life because they require a great amount of operations per inference. Not mentioning the number of operations from the result is misleading for the AI community and the public. It creates an unreal expectation that these models are applicable in real life. In this thesis, we want to come up with a state of the art solution that requires a low number of floating point operations per inference. Therefore, bridging the gap between expectations and reality. We will answer,
\begin{quote}
How can we reduce the inference complexity of Neural Networks?
How do these modifications effect accuracy?
\end{quote}
First, we will briefly describe neural networks and some of their underlying building blocks. We will mention the complexities of necessary operations. Then, we will provide solutions to reduce these complexities.

\section{Neural Networks}
In this chapter, we will try to describe neural networks briefly. We will provide some terminology and give some examples. 

Neural networks are \textit{weighted graphs}. They consist of an ordered set of \textit{layers}, where every layer is a set of \textit{nodes}. The first layer of the neural network is called the \textit{input layer}, and the last one is called the \textit{output layer}. The layers in between are called \textit{hidden layers}. In our case, nodes belonging to one layer are connected to the nodes in the following and/or the previous layers. These connections are weighted edges, and they are mostly called as \textit{weights}. 

Given an input, neural networks nodes have \textit{outputs}, which are real numbers. The output of a node is calculated by applying a function ($\psi$) the outputs of the nodes belonging to previous layers . Preceding that, the output of the input layer is calculated using the input data (see Eq. \ref{eq:output_of_layers}).  By calculating the layer outputs consecutively we calculate the output of the output layer. This process is called \textit{inference}. We use the following notations to denote the concepts that we just explained.
\begin{equation}
\label{eq:variable_definitions}
\begin{split}
l_k & \text{: a column vector of nodes for layer $k$}\\
m_k & \text{: the number of nodes in $l_k$}\\
l_{k,i}  & \text{: node $i$ in $l_k$}\\
o_{k}  & \text{: the output vector representing the outputs of nodes in $l_{k}$}\\
o_{k,i}  & \text{: the output of $l_{k,i}$}\\
\mathbf{w}^{(k)}  & \text{: weight matrix connecting nodes in $l_{k-1}$ to nodes in $l_{k}$} \\
w^{(k)}_{i,j}  & \text{: the weight connecting nodes $l_{(k-1),i}$ and $l_{k,j}$} \\
\mathbf{b}^{(k)}  & \text{: the bias term for $l_{k}$} \\
\psi_k & \text{: function to determine $o_k$ given $o_{k-1}$}\\
\sigma & \text{: activation functions} \\
\mathbf{x} & \text{: all inputs of the dataset, consisting of $N$ data points} \\
\mathbf{y} & \text{: all outputs of the dataset} \\
\mathbf{\hat y} & \text{: approximation of the output}  \\
x_n & \text{: $n$th input data ($0 < n \leq N$)} \\
y_n & \text{: $n$th output data ($0 < n \leq N$)} \\
\hat y_n & \text{: approximation of $y_n$ given $x_n$ ($0 < n \leq N$)}\\
\text{FC} & \text{: stands for Fully Connected (e.g. $\psi^{(FC)}$)}
\end{split}
\end{equation}
Therefore the structure of a neural network is determined by the number of layers and the functions that determine the outputs of layers.
\begin{equation}
\label{eq:output_of_layers}
    o_k = 
\begin{cases}
    \psi(o_{k-1}), &\text{if } k\geq 1\\
    \mathbf{x},& k = 0\\
\end{cases}
\end{equation}

\subsection{Fully Connected Layers}
As the name suggests, for two consecutive layers to be \textit{fully connected}, all nodes in the previous layer must be connected to all nodes in the following layer. 

Let's assume two consecutive layers, $l_{k-1}$ and $l_{k}$, with shapes $m_{k-1} \times 1$ and $m_k \times 1$, respectively. For these layers to be fully connected, the weight matrix $\mathbf{w}^{(k)}$, should be of shape $m_{k-1} \times m_{k}$. Most fully connected layers also include a bias term ($m$) for every node $l_k$. In fully connected layers, $o_k$ would simply be calculated using layer function $\psi^{(FC)}$.
$$ \psi^{(FC)}_k(o_{k-1}) = o_{k-1}^T\mathbf{w}^{(k)} + \mathbf{b}^{(k)}$$
Therefore the computational complexity of $\psi^{(FC)}$ would become
$$\mathcal{O}(\psi^{(FC)}_k) = \mathcal{O}(m_{k-1}m_{k})$$

\subsection{Activation Function and Nonlinearity}
By stacking fully connected layers, we can increase the depth of a neural network. By doing so we want to increase approximation quality of the neural network. However, the $\psi^{(FC)}$ we have defined is a linear function. Therefore if we stack multiple fully connected layers we would end up with a linear model. 

To achieve non-linearity, we apply \textit{activation functions} to the results of $\psi$. There are many activation functions (such as $tanh$ or $sigmoid$) but one very commonly used activation function is $ReLU$.  
\begin{equation}
\label{eq:relu_definition}
    ReLU(x) = 
\begin{cases}
    x, & \text{if }x \geq 0\\
    0 &  \text{otherwise }\\
\end{cases}
\end{equation}
Therefore we will redefine the fully connected $\psi^{(FC)}$ as;

$$ \psi^{(FC)}_k(o) = \sigma(o^T\mathbf{w}^{(k)} + \mathbf{b}^{(k)})$$

Note that the activation function does not strictly belong to the definition of a fully connected layer. But for simplicity, we are going to include them in the layer functions ($\psi$).

$\psi^{(FC)}$ is one of the most basic building blocks of any Neural Network. Stacking $K$ of them after the input, we can try to approximate an output given an input. To do that we will calculate the outputs of every layer, starting from the input. 
\begin{equation*}
\begin{split}
o_0 &= x_n\\
o_1 &= \psi_1^{(FC)}(o_0)\\ 
o_2 &= \psi_2^{(FC)}(o_1)\\
...&\\
o_K &= \psi_1^{(FC)}(o_{K-1})\\
\hat y &= o_K
\end{split}
\end{equation*}

\subsection{Loss}
To represent the quality of an approximation, we are going to use a loss (or cost) function. A good example would be the loss of a salesman. Assuming a customer who would pay at most \$10 for a given product, if the salesman sells this product for \$4, the salesman would face a loss of \$6 from his potential profit. Or if the salesman tries to sell this product for \$14, the customer will not purchase it and he will face a loss of \$10. In this example, the salesman would want to minimize the loss to earn as much as possible. 

A commonly used loss function is Root Mean Square Error (RMSE). Given an approximation ($\hat y$) and the corresponding output ($y$), RMSE can be calculated as,
\begin{equation*}
L = RMSE(\hat y, y) = \sqrt{\frac{\sum^N_{n=1} (\hat y_n - y_n)^2 }{N}}
\end{equation*}
There are two common properties of loss functions. First, loss is never negative. Second, if we compare two approximations, the one with a smaller loss is better.

If all our approximations are exactly the same as the output ($  \mathbf{\hat y} =  \mathbf{y} $), the total loss would be 0. 

\subsection{Stochastic Gradient Descent}
To provide better approximations, we will try to optimize the neural network parameters. One common way to optimize these parameters is to use Stochastic Gradient Descent (SGD). SGD is an iterative learning method that starts with some initial (random) parameters. Given $\theta \in (\mathbf{w} \cup \mathbf{b}$) to be a parameter that we want to optimize. The learning rule updating theta would be;

$$ \theta = \theta- \eta \nabla_\theta{L(f(x), y)} $$

where $\eta$ is the learning rate, and $\nabla_\theta{L(f(x), y)}$ is the partial derivative of the loss in terms of given parameter, $\theta$. One iteration is completed when we update every parameter for given example(s). By performing many iterations, SGD aims to find a global minimum for the loss function, given data and initial parameters.

\subsection{Convolutional Layer}
So far we have seen the key elements we can use to create and train fully connected neural networks. To be able to apply neural networks to image inputs, we will use convolutional layers or convolution operation. Please note that we are assuming one or two dimensional convolutions with same padding. 

Let's assume a 3 dimensional layer output $o_{k-1} \in \mathbb{R}^{W_{k-1} \times H_{k-1} \times m_{k-1}}$. Convolution operation first creates a sliding window that goes through the layer. The contents of this sliding window are patches ($p_{k-1,(I,J)} \in \mathbb{R}^{K \times K \times m_{k-1}}$). By multiplying a weight matrix $\mathbf{w}^{(k)} \in \mathbb{R}^{K \times K \times m_{k-1} \times m_k}$ with every patch, it creates a set of output nodes $o_{(k+1),(I,J)} \in \mathbb{R}^{1 \times m_k}$. Those output nodes represent the features belonging to the pixel at point $(I,J)$. By performing this operation for every patch, we calculate the outputs of a convolutional layer. While calculating the patches, we also make use of a parameter called stride, $s_k \in \mathbb{N}^+$. $s$ defines the number of vertical and horizontal indexes between each patch.

$$ W_k = \floor*{\frac{W_{k-1}}{s_k}}$$
$$ H_k = \floor*{\frac{H_{k-1}}{s_k}}$$
$$ \psi_{k}^{(Conv)} : \mathbb{R}^{W_{k-1} \times H_{k-1} \times m_{k-1} } \rightarrow \mathbb{R}^{ W_k \times H_k \times m_k} $$
$$ p_{k-1,(I,J)} \in \mathbb{R}^{K \times K \times m_{k-1}} $$
$$ p_{k-1,(I,J)} \subset o_{k-1}$$
$$ p_{k-1,(I,J)} = (p_{k-1,(I,J),(i,j)}) $$
$$ p_{k-1,(I,J),(i,j)} \in \mathbb{R}^{m_k-1} $$
$$ p_{k-1,(I,J),(i,j)} = o_{k-1,(a,b)} $$
where
$$ o_{k-1} = (o_{k-1,(a,b)})$$
$$ a = Is + (i - \floor*{K/2}) $$
$$ b = Js + (j - \floor*{K/2}) $$
and 
$$ 0 < I \leq W_k$$
$$ 0 < J \leq H_k$$

$$\psi_{k}^{(Conv)}(o_{k-1}) = (\sigma(p_{k-1,(I,J)} \mathbf{w}^{(k)} + \mathbf{b}^{(k)})) $$


\subsection{Pooling}
Pooling is a way of reducing the dimensionality of an output. Depending on the task, one may choose from different pooling methods. Pooling methods also create patches $p_{k-1,(I,J)} \in \mathbb{R}^{K \times K \times m_{k-1}}$ with strides $s$. But this time, instead of applying a weight, bias and activation function, they apply different functions. 
The function that we will make use of is $ M : \mathbb{R}^{K \times K \times m} \rightarrow \mathbb{R}^{m}$. By defining different variations of M, we will define \textit{max pooling} and \textit{average pooling}.
\subsubsection{Max Pooling}
Max pooling takes the maximum value in a channel within the patch.
$$ M^{(max)}(p) = \{max(\{p_{i,j,l} \ |\  i \in [1, \ldots, K],  j \in [1, \ldots, K] \}\}) \ |\  l \in [1, \ldots, m] \} $$
\subsubsection{Average Pooling}
Average pooling averages the values within the patch per channel. 
$$ M^{(avg)}(p) = \{\sum_{i=1}^{K}\sum_{j=1}^{K}\frac{p_{i,j,m}}{K^2} \ | \  l \in [1, \ldots, m] \} $$



\section{Efficient Operations}
In this section we are going to look at some ways to reduce the computational complexities of fully connected layers and convolutional layers. 

\section{Factorization}
Factorization is approximating a weight matrix using smaller matrices. This has interesting uses with Neural Networks. Assume that we have a fully connected layer $k$. Using factorization, we can approximate $\mathbf{w}^{(k)} \in \mathbb{R}^{m_{k-1} \times m_k}$ using two smaller matrices, $U_{\mathbf{w}^{(k)}} \in \mathbb{R}^{m_{k-1} \times n}$ and $V_{\mathbf{w}^{(k)}} \in \mathbb{R}^{n \times m_{k}}$. If we can find matrices such that $U_{\mathbf{w}^{(k)}}V_{\mathbf{w}^{(k)}} \approx \mathbf{w}^{(k)}$, we can rewrite, 
$$\psi^{(FC)}_k(o) \approx \psi'^{(FC)}_k(o) = \sigma(o^T U_{\mathbf{w}^{(k)}}V_{\mathbf{w}^{(k)}} +\mathbf{b}^{(k)})$$
Therefore, we can reduce the complexity of layer $k$ by setting $n$. As we have mentioned before, $\mathcal{O}(\psi_k^{(FC)}) = \mathcal{O}(m_{k-1}m_k)$. When we approximate this operation, the complexity becomes, 
$$\mathcal{O}(\psi'^{(FC)}_k) = \mathcal{O}(n(m_{k-1}+m_k))$$
Therefore, if there is a good enough approximation, satisfying $n < \frac{m_{k-1}m_k}{m_{k-1}+m_k}$, we can reduce the complexity of a fully connected layer without effecting the results.
One thing that's similar between a convolutional layer and a fully connected layer is that both are performing matrix multiplication to calculate results. The only difference is, a convolutional layer is possibly performing this matrix multiplication many times. Therefore the same technique can be used with convolutional layers. 


\todoin{Luc says: If XW is not exactly equal XOP, how will you deal with this?}
\subsection{SVD}
Singular Value Decomposition (SVD) (\cite{golub1970singular}), is a factorization method that we can use to calculate this approximation. SVD decomposes the weight matrix $\mathbf{w}^{(k)} \in \mathbb{R}^{m_{k-1} \times m_k}$ into $3$ parts. 
$$ \mathbf{w}^{(k)} = USV $$
Where, $U \in \mathbb{R}^{m_{k-1} \times m_{k-1}}$ and $V\in \mathbb{R}^{m_{k} \times m_k}$. And $S \in \mathbb{R}^{m_{k-1} \times m_k}$ is a rectangular diagonal matrix. The diagonal values of $S$ are called as the singular values of $M$. Selecting the $n$ highest values from $S$ and corresponding columns and rows from $U$ and $V$, respectively, lets us create a \textit{low-rank decomposition} of $\mathbf{w}^{(k)}$. 
$$ \mathbf{w}^{(k)} \approx U'S'V' $$
where $U' \in \mathbb{R}^{m_{k-1} \times n}$, $V' \in \mathbb{R}^{n \times n}$, and $S' \in \mathbb{R}^{n \times m_k}$. By choosing a sufficiently small $n$ value and setting $U_{\mathbf{w}^{(k)}}=U'S'$ and $V_{\mathbf{w}^{(k)}} = V'$, we can approximate the weights, and reduce the complexity of a layer.

\subsection{Weight Sharing}
Weight sharing assumes we have a weight values for a layer, $\mathbf{W}^{(k)} \in \mathbb{R}^{a}$. Instead of having a weight matrix, we have a matrix of indices $d^{(k)} \in \mathbb{N}^{m_{k-1} \times m_k}$ satisfying $d^{(k)}_{i,j} < a$. By defining $\mathbf{w}^{(k)}_{i,j} = \mathbf{W}^{(k)}_{d^{(k)}_{i,j}}$ and defining a sufficiently small number of elements, we can compose the weights using a limited number of real numbers and some natural number indices. If we choose the size of this real numbers set, 

\todoin{give some examples here, you're saying some papers.}

\section{Convolution Operation Alternatives}

\subsection{Convolutional Composition}
As  \cite{alvarez2016decomposeme} explains, a convolution operation with a weight matrix $\mathbf{w}^{(k)} \in \mathbb{R}^{K \times K \times m_{k-1} \times m_k}$, could be composed using two convolution operations with kernels $\mathbf{w}^{(k_1)} \in \mathbb{R}^{1 \times K \times m_{k-1} \times n}$ and $\mathbf{w}^{(k_2)} \in \mathbb{R}^{K \times 1 \times n \times m_k}$. Their technique, instead of factorizing learned weight matrices, aims to learn the factorized kernels. They also aim to increase non-linearity by adding bias and activation function in between. Therefore defining;

$$ \psi_{(k)}^{(ConvCompose)}(o) = \psi_{(k_2)}^{(Conv)}(\psi_{(k_1)}^{(Conv)}(o))$$

This method forces the separability of the weight matrix as a hard constraint. By performing such an operation, they convert the computational complexity of a convolution operation from $\mathcal{O}(KKm_{k-1}m_k)$ to $\mathcal{O}(Kn(m_{k-1} +m_{k}))$. Suggesting that, if we can find an $n$ satisfying $\frac{\mathcal{O}(KKm_{k-1}m_k)}{\mathcal{O}(Kn(m_{k-1} +m_{k})} > 1$, we can reduce the complexity of this layer. This equation can be rewritten as;
$$ \frac{Km_{k-1}m_k}{m_{k-1} + m_k} > n$$


\subsection{Separable Convolutions}
Separable convolutions separate the standard convolution operation into two parts. These parts are called depthwise convolutions and pointwise convolutions. Depthwise convolution applies a given number of filters on every input channel, one by one therefore results with output channels equal to input channel times number of filters. 
\subsubsection{Depthwise Convolution}
Given a patch $p \in \mathbb{R}^{K \times K \times m}$, depthwise convolution has a weight matrix $\mathbf{w}^{(k, depthwise)} \in \mathbb{R}^{K \times K \times m}$. For easiness, let's assume a variant of $p$ and $\mathbf{w}^{(k, depthwise)}$ as in,
$$\mathbf{w}'^{(k, depthwise)}_m = \{ \mathbf{w}^{(k, depthwise)}_{i,j,m} \ | \ i \in [1, \ldots, K], j \in [1, \ldots, K] \}$$
$$p'_m = \{ p_{i,j,m} \ | \ i \in [1, \ldots, K], j \in [1, \ldots, K] \}$$
Therefore, depthwise convolution operation can be defined as, 
$$\psi^{(k, depthwise)}(p) = \{ p'_m\mathbf{w}'^{(k, depthwise)}_m \ | \ i \in [1, \ldots, m] \} $$
\subsubsection{Pointwise Convolution}
Pointwise convolution is the regular convolution operation with kernel size 1 ($K = 1$). 



To describe the complexity of this operation, let's assume that we have a separable convolution with kernel size $N$, input channels $K$, depthwise filters $I$ and output channels $L$. First operation will be applying $L$ filters with size $N \times N$ to $K$ input channels, one by one. The number of operations we need for this operation is, $IKN^2$. Second operation will be multiplying $1 \times IK$ output with $IK \times L$ correlation matrix, requiring $IKL$ floating point operations. In total we need $IK(N^2+L)$ operations. 



\section{Pruning}
Pruning aims to reduce the number of operations by deleting the parameters that has low or no impact in the result. Studies show that applying this method in an ANN is effective in reducing the model complexity, improving generalization, and they are effective in reducing the required training cycles. In our experiments we will try to reproduce these effects.
To visualize these methods, and help with the explanation later, let's think of two fully connected layers, $\mathbf{l_1}$ and $\mathbf{l_2}$. $\mathbf{l_1}$ is the input of this operation and it consists of $N$ values, $\mathbf{l_1}=(l_{11}, l_{12}, ..., l_{1N})$. $\mathbf{l_2}$ is the output of this operation consists of $M$ values, $\mathbf{l_2}=(l_{21}, l_{22}, ..., l_{2M})$. Between these two layers, there is a weight matrix $W$ with size $N \times M$. The operation, that we want to optimize is, $\mathbf{l_2} = \mathbf{l_1}W$. To do so, we will look at 2 cases of pruning. One will be focusing on pruning individual weights, and the other will be focusing on removing unimportant rows and columns from $\mathbf{l_2}$, $\mathbf{l_1}$ and $W$. 
\todoin{maybe explain in more detail and give examples of pruning algorithms here. (e.g. Optimal Brain Damage, Second order derivatives for network pruning: Optimal Brain Surgeon, Optimal Brain Surgeon and general network pruning, SEE Pruning Algorithms-a survey from R. Reed)}

\subsection{Pruning Weights}
This subcategory of pruning algorithms try to optimize the number of floating point operations by removing some individual values from $W$. Theoretically, it could benefit the computational complexity to remove individual scalars from W, by not performing operations related to those weights. But practically, in Tensorflow, matrix multiplication on dense matrices uses all of the values of it's inputs. In contrary, sparse matrix multiplication takes a sparse matrix and a dense matrix as inputs and outputs a dense matrix. To implement this method, we could convert W to a sparse tensor after pruning the weights. But, Tensorflow documentations explicitly state;
\begin{displayquote}
\begin{itemize}
\item Will the SparseTensor $A$ fit in memory if densified?
\item Is the column count of the product large ($>> 1$)?
\item Is the density of $A$ larger than approximately $15\%$?
\end{itemize}
"If the answer to several of these questions is yes, consider converting the SparseTensor to a dense one."
\end{displayquote}
In our terms, SparseTensor $A$ is corresponding to the pruned version of $W$. Since $W$ was already dense before, we can assume that the answer to the first question is yes. The column count of our product is $M$ which is much larger than $1$ in some cases. Also we don't know anything about the density of pruned version of $W$. Looking at these facts, we are assuming that implementing this operation will be problematic. Instead of delving deeper into these problems to evaluate this method, we will move on to other methods.
\todoin{add figure to show what happens when we prune}
\subsection{Activation Based Pruning - Fully Connected Layers} \label{sec:activation-based-pruning-convolution}
Activation based pruning, works by looking at individual values in layers, and prunes the layer and corresponding weight row/columns completely. To visualize this, we will assume that the fully connected layers we have defined are, trained to some extent, and activated using ReLU activations. With this definition, if we apply our dataset and count the number of activations in $\mathbf{l_1}$ and $\mathbf{l_2}$, we may realize that there are some neurons that are not being activated at all. By removing these neurons from the layers, we can reduce the number of operations. This removal operation is done by removing neurons based on their activations. 
\todoin{add figure to show what happens when we prune}

\subsection{Activation Based Pruning - Convolution and Deconvolutions}
\todoin{Put references for conv and deconv operations. }
In theory, convolution operation is a matrix multiplication applied on a sliding window. Thus, counting the output feature activations of a convolution operation, we can apply activation based pruning. 

\subsection{Second Order Derivatives (Fischer Information Matrix)}

\section{Quantization}

\subsection{8-bit Quantization}
\subsection{n-bit Quantization}

\section{Efficient Structures}
Some structures help neural networks represent more information using less parameters.
\todoin{talk more about why some structures are more efficient, how they help with training speed, how they reduce the number parameters or number of floating point operations even while increasing the accuracy.}

\subsection{Inception Blocks}
\todoin{do the introduction to \cite{Szegedy:2014aa} and how it is improved using \cite{Szegedy_2016_CVPR}}
\subsection{Bottleneck Blocks}
\cite{He:2015aa} introduced residual connections with bottleneck blocks. To optimize the performance of their network, they have introduced the bottleneck blocks. Bottleneck blocks contain 3 convolutions. First is a $ 1 \times 1$ convolution that scales down the input channels to half. Output is applied to a $3 \times 3$ convolution which doesn't change the number of channels, and following that with a $1 \times 1$ convolution to quadruple the number of input channels. As an example, we can look at the conv3\_x block of 50-layer network described in Table \ref{tab:bottleneck-comparison}.
\todoin{try to reason why bottleneck blocks work. Luc says: what is the reasoning of this? why would one want to do this?}

\cite{He:2015aa} compared the performance of various network configurations on ImageNet validation dataset. From these comparisons, we have selected the 34-layer network and the 54-layer network. The 34-layer network is consisting of pairs of $3 \times 3$ blocks. The 50-layer network is consisting of bottleneck blocks. In table \ref{tab:bottleneck-comparison} we have compared these networks by their structure, required number of FLOPs, and their top-1 and top-5 errors on this dataset. As we can see in the FLOPs, the networks have about $5\%$ of difference in number of floating point operations. As \cite{He:2015aa} reports, this small increase in parameters is effecting accuracy of the model considerably. 

\begin{table}[]
\centering
\begin{tabular}{ | c | c | c | c | }
\hline
layer name			& output size 					& 34-layer																& 50-layer																			\\ \hline
input image			& $224 \times 224$				& \multicolumn{2}{c|}{}																																	\\ \hline
conv1				& $112 \times112$				& \multicolumn{2}{c|}{$ 7 \times 7$, $64$, stride $2$}																												\\ \hline
\multirow{2}{*}{conv2\_x}	& \multirow{2}{*}{$56 \times 56$} 	& \multicolumn{2}{c|}{$3 \times 3$ max pool, stride $2$}																											\\ \cline{3-4} 
					&							& $\begin{bmatrix} 3 \times 3, &   64 \\ 3 \times 3, &   64 \end{bmatrix} \times 3 $		& $\begin{bmatrix}1 \times 1, & 64 \\ 3 \times 3, & 64 \\ 1 \times 1, & 256 \end{bmatrix}^{} \times 3 $ 		\\ \hline
conv3\_x				& $28 \times 28$				& $\begin{bmatrix} 3 \times 3, & 128 \\ 3 \times 3, & 128 \end{bmatrix} \times 3 $		& $\begin{bmatrix}1 \times 1, & 128 \\ 3 \times 3, & 128 \\ 1 \times 1, & 512 \end{bmatrix} \times 3$		\\ \hline
conv4\_x				& $14 \times 14$				& $\begin{bmatrix} 3 \times 3, & 256 \\ 3 \times 3, & 256 \end{bmatrix} \times 3 $		& $\begin{bmatrix}1 \times 1, & 256 \\ 3 \times 3, & 256 \\ 1 \times 1, & 1024 \end{bmatrix} \times 3$		\\ \hline
conv5\_x				& $  7 \times   7$				& $\begin{bmatrix} 3 \times 3, & 512 \\ 3 \times 3, & 512 \end{bmatrix} \times 3 $		& $\begin{bmatrix}1 \times 1, & 512 \\ 3 \times 3, & 512 \\ 1 \times 1, & 2048 \end{bmatrix} \times 3$		\\ \hline
					& $  1 \times   1$				&\multicolumn{2}{c|}{average pool, 1000-d fc, softmax}																											\\ \hline
\multicolumn{2}{| c |}{FLOPs}							& $3.6 \times 10^9$														& $3.8 \times 10^9$																	\\ \hline
\multicolumn{2}{| c |}{top-1 error ($\%$)}						& $21.53$																& $20.74$																			\\ \hline
\multicolumn{2}{| c |}{top-5 error ($\%$)}						& $5.60$																& $5.25$																			\\ \hline
\multicolumn{2}{| c |}{top-1 error \small{($\%$, \textbf{10-crop} testing)}}						& $24.19$																& $22.85$																			\\ \hline
\multicolumn{2}{| c |}{top-5 error \small{($\%$, \textbf{10-crop} testing)}}						& $7.40$																& $6.71$																			\\ \hline
\end{tabular}
\caption{Comparison of bottleneck blocks (50-layer) with stacked $ 3 \times 3$ layers (34-layer). }
\label{tab:bottleneck-comparison}
\end{table}

But the main contribution of \cite{He:2015aa} is not the bottleneck architecture, but Residual Connections that we will see in another section. 


\section{Improving Network Efficiency}
\subsection{Residual Connections}
\cite{He:2015aa} is also introducing a method called Residual Connections.
\todoin{go into details of how this works using information given in \cite{He:2015aa} and \cite{DBLP:journals/corr/SzegedyIV16}}
\subsection{Batch Normalization}
\begin{equation}
\begin{split}
V \approx& HW \\
IV \approx& IHW\\
I \in& \mathbb{R}^{l \times m}
\end{split}
\end{equation}


\section{Datasets}


























