% !TEX root = ../thesis.tex
\iffalse
Taken from http://www.ldeo.columbia.edu/~martins/sen_sem/thesis_org.html
Methods
What belongs in the "methods" section of a scientific paper?
    Information to allow the reader to assess the believability of your results.
    Information needed by another researcher to replicate your experiment.
    Description of your materials, procedure, theory.
    Calculations, technique, procedure, equipment, and calibration plots. 
    Limitations, assumptions, and range of validity.
    Desciption of your analystical methods, including reference to any specialized statistical software. 
The methods section should answering the following questions and caveats: 
    Could one accurately replicate the study (for example, all of the optional and adjustable parameters on any sensors or instruments that were used to acquire the data)?
    Could another researcher accurately find and reoccupy the sampling stations or track lines?
    Is there enough information provided about any instruments used so that a functionally equivalent instrument could be used to repeat the experiment?
    If the data are in the public domain, could another researcher lay his or her hands on the identical data set?
    Could one replicate any laboratory analyses that were used? 
    Could one replicate any statistical analyses?
    Could another researcher approximately replicate the key algorithms of any computer software?
Citations in this section should be limited to data sources and references of where to find more complete descriptions of procedures.
Do not include descriptions of results. 
\fi
\iffalse
We need to be talking about these.
    Quantization
        8-bit (known to work in mobile, we will test it in the very end)
        n-bit (not possible with Tensorflow)
    Pruning,
        by weight (done)
        by neuron activations (done)
        by Fisher Information Metric (skipped because hard to implement)
    Different Types of Operators,
        ef-operation (http://arxiv.org/abs/1702.02676) (not possible with Tensorflow)
        separable convolutions (done)
    Factorization,
        by SVD (done)
        weight sharing (not possible with Tensorflow)
        non-negative matrix factorization (not good in theory)
    Different Structures,
        bottleneck blocks (reported)
        inception blocks (to be reported)
        1D convolutions (done)
        spiking neural network (not possible)
    Improving Network Efficiency
        Residual Connections
        Batch Normalization
\fi

\section{Quantization}

\subsection{8-bit Quantization}
\subsection{n-bit Quantization}
\section{Pruning}
Pruning aims to reduce the number of operations by deleting the parameters that has low or no impact in the result. Studies show that applying this method in an ANN is effective in reducing the model complexity, improving generalization, and they are effective in reducing the required training cycles. In our experiments we will try to reproduce these effects.
To visualize these methods, and help with the explanation later, let's think of two fully connected layers, $\mathbf{l_1}$ and $\mathbf{l_2}$. $\mathbf{l_1}$ is the input of this operation and it consists of $N$ values, $\mathbf{l_1}=(l_{11}, l_{12}, ..., l_{1N})$. $\mathbf{l_2}$ is the output of this operation consists of $M$ values, $\mathbf{l_2}=(l_{21}, l_{22}, ..., l_{2M})$. Between these two layers, there is a weight matrix $W$ with size $N \times M$. The operation, that we want to optimize is, $\mathbf{l_2} = \mathbf{l_1}W$. To do so, we will look at 2 cases of pruning. One will be focusing on pruning individual weights, and the other will be focusing on removing unimportant rows and columns from $\mathbf{l_2}$, $\mathbf{l_1}$ and $W$. 
\todoin{maybe explain in more detail and give examples of pruning algorithms here. (e.g. Optimal Brain Damage, Second order derivatives for network pruning: Optimal Brain Surgeon, Optimal Brain Surgeon and general network pruning, SEE Pruning Algorithms-a survey from R. Reed)}

\subsection{Pruning Weights}
This subcategory of pruning algorithms try to optimize the number of floating point operations by removing some individual values from $W$. Theoretically, it could benefit the computational complexity to remove individual scalars from W, by not performing operations related to those weights. But practically, in Tensorflow, matrix multiplication on dense matrices uses all of the values of it's inputs. In contrary, sparse matrix multiplication takes a sparse matrix and a dense matrix as inputs and outputs a dense matrix. To implement this method, we could convert W to a sparse tensor after pruning the weights. But, Tensorflow documentations explicitly state;
\begin{displayquote}
\begin{itemize}
\item Will the SparseTensor $A$ fit in memory if densified?
\item Is the column count of the product large ($>> 1$)?
\item Is the density of $A$ larger than approximately $15\%$?
\end{itemize}
"If the answer to several of these questions is yes, consider converting the SparseTensor to a dense one."
\end{displayquote}
In our terms, SparseTensor $A$ is corresponding to the pruned version of $W$. Since $W$ was already dense before, we can assume that the answer to the first question is yes. The column count of our product is $M$ which is much larger than $1$ in some cases. Also we don't know anything about the density of pruned version of $W$. Looking at these facts, we are assuming that implementing this operation will be problematic. Instead of delving deeper into these problems to evaluate this method, we will move on to other methods.
\todoin{add figure to show what happens when we prune}
\subsection{Activation Based Pruning - Fully Connected Layers} \label{sec:activation-based-pruning-convolution}
Activation based pruning, works by looking at individual values in layers, and prunes the layer and corresponding weight row/columns completely. To visualize this, we will assume that the fully connected layers we have defined are, trained to some extent, and activated using ReLU activations. With this definition, if we apply our dataset and count the number of activations in $\mathbf{l_1}$ and $\mathbf{l_2}$, we may realize that there are some neurons that are not being activated at all. By removing these neurons from the layers, we can reduce the number of operations. This removal operation is done by removing neurons based on their activations. 
\todoin{add figure to show what happens when we prune}

\subsection{Activation Based Pruning - Convolution and Deconvolutions}
\todoin{Put references for conv and deconv operations. }
In theory, convolution operation is a matrix multiplication applied on a sliding window. Thus, counting the output feature activations of a convolution operation, we can apply activation based pruning. 

\subsection{Fisher Information Metric}

\section{Efficient Operations}
\subsection{Separable Convolutions}
As the name suggests, these operations separate the standard convolution operation into two parts. These parts are called Depthwise convolutions and Pointwise convolutions. Depthwise convolution applies a given number of filters on every input channel, one by one therefore results with output channels equal to input channel times number of filters. Pointwise convolution correlates these output channels with each other, or in other words mixes them, by applying a matrix multiplication. For example, let's assume we have an input with 3 channels, applying a Depthwise convolution with 4 filters to that, we would get 12 output channels. Then applying a Pointwise convolution to that, we correlate these 12 output channels to create new output channels. 

To describe the complexity of this operation, let's assume that we have a separable convolution with kernel size $N$, input channels $K$, depthwise filters $I$ and output channels $L$. First operation will be applying $L$ filters with size $N \times N$ to $K$ input channels, one by one. The number of operations we need for this operation is, $IKN^2$. Second operation will be multiplying $1 \times I$ output with $I \times L$ correlation matrix, requiring $IL$ floating point operations. In total we need $I(KN^2+L)$ operations. 
\subsection{ef-operator}

\section{Factorization}
Using Factorization methods, we can decompose a matrix into smaller different matrices. Some factorization methods can be used to reduce the dimensionality of these smaller matrices while approximating the original matrix. This has interesting uses with Neural Networks. Assume that we have a matrix multiplication operation. We are multiplying a random input $\mathbf{X}$ with a fixed weight matrix $\mathbf{W}$. Let's say $\mathbf{X}$ is $K \times N$ and $\mathbf{W}$ is $N \times M$. The matrix multiplication of these two matrices has the complexity of $KNM$. If we can successfully decompose $\mathbf{W}$ to the composition of two matrices $\mathbf{O}$ and $\mathbf{P}$ with dimensions $N \times L$ and $L \times M$, respectively, we can rewrite our matrix multiplication operation as; $\mathbf{X}\mathbf{W} \approx \mathbf{X}\mathbf{O}\mathbf{P}$. The new complexity of this operation would be $KNL + NLM = NL(K+M)$. If we can find a decomposition where $L$ is sufficiently small that successfully approximates the matrix $\mathbf{W}$ and satisfies $NL(K+M) < KNM$, we can reduce the complexity of this matrix multiplication.
\todoin{draw some figures explaining how this happens}
\todoin{Luc says: If XW is not exactly equal XOP, how will you deal with this?}
\subsection{SVD}
Using SVD we can make this approximation. SVD decomposes the a matrix into 3 parts. 
\todoin{Talk what SVD does, what are the decomposed matrices, what are the singular values and how they are relevant to the approximation of $\mathbf{W}$. Show your experiments and results from when you ran the experiments.} 


\subsection{Weight Sharing}
Weight sharing assumes we have a limited set of weights, and when we are representing values, instead of representing the value itself, we represent the indices to weights. This operation is used in some papers successfully to reduce model size considerably. To be able to implement such a representation, we can use 2 potential functions of Tensorflow. One is \texttt{tf.gather} which selects the given indices from a given matrix. The other is \texttt{tf.embedding\_lookup} which works with a bucket of values and returns the given keys/indices from the given bucket. However, both methods accept the indices as a matrix of 32-bit or 64-bit integers. Therefore storing indices instead of weights would not yield with any improvements in the model size. Without an implementation of these methods using low-bit integers, it is not possible to exploit their usefulness.
\todoin{give some examples here, you're saying some papers.}

\subsection{Other Factorization Methods}

\section{Efficient Structures}
Some structures help neural networks represent more information using less parameters.
\todoin{talk more about why some structures are more efficient, how they help with training speed, how they reduce the number parameters or number of floating point operations even while increasing the accuracy.}
\subsection{1D Convolutions}
Convolution kernels could be decomposed into smaller operations. For example using SVD, we can decompose the convolution kernel into the multiplication for two smaller kernels, and by applying these kernels consequently, we can approximate the convolution operation with fewer floating point operations. Instead of doing separating learned convolutions, we are going to use the method introduced in \cite{alvarez2016decomposeme}. This method forces the separability of convolution operation as a hard constraint. 

Normally convolution operations are defined 2 dimensional ($2D$). That is, kernel sizes are ($2D$). For example convolutions we have used in Section \ref{sec:activation-based-pruning-convolution} are $2D$, their kernel sizes are $3 \times 3$. With this method, we are aiming to construct a $N \times N$ convolution operation as a combination two convolution operations. The first convolution has kernel size $1 \times N$ and the following convolution has $N \times 1$, or vice versa. Looking back at the experiment we did in  Section \ref{sec:activation-based-pruning-convolution}, instead of applying one $3 \times 3$ convolution, we are talking about applying one $1 \times 3$ convolution followed by a $3 \times 1$ convolution. 

The amount of speed up can be approximated using the number of floating point operations. A convolution operation can be seen as a matrix multiplication applied to consequent subsections of an image. To visualize this, let's assume that we have a convolution operation with kernel size $N$, input channels $K$ and output channels $L$. Assuming we are applying that operation to a $N \times N$ image patch with $K$ channels, our operation can be simplified as multiplying this $N*N*K$ vector with $N*N*K \times L$ matrix. The number of floating point operations required to execute this operation are $N^2KL$. When composed as two $1D$ compositions, this operation will be represented with two $1D$ convolutions. First convolution is multiplying vector $N*K$ with matrix $N*K \times P$ and the second convolution is multiplying vector $N*P$ with matrix $N*P \times L$. As you can see we have introduced the variable $P$ as the intermediate output of the first convolution. Number of floating point operations required for these operations are, $NLP$ and $NPK$. And summing them up, we can say we need $NP(L + K)$ operations in total. To see if this decomposition is reducing the number of operations, we could basically try to define a $P$ satisfying $\frac{NP(L+K)}{N^2KL} < 1$. Therefore,
\begin{equation*}
1 \leq P < \frac{NKL}{L+K}
\end{equation*}


To show the validity of this method, we have conducted two experiments. 

\subsection{Inception Blocks}
\todoin{do the introduction to \cite{Szegedy:2014aa} and how it is improved using \cite{Szegedy_2016_CVPR}}
\subsection{Bottleneck Blocks}
\cite{He:2015aa} introduced residual connections with bottleneck blocks. To optimize the performance of their network, they have introduced the bottleneck blocks. Bottleneck blocks contain 3 convolutions. First is a $ 1 \times 1$ convolution that scales down the input channels to half. Output is applied to a $3 \times 3$ convolution which doesn't change the number of channels, and following that with a $1 \times 1$ convolution to quadruple the number of input channels. As an example, we can look at the conv3\_x block of 50-layer network described in Table \ref{tab:bottleneck-comparison}.
\todoin{try to reason why bottleneck blocks work. Luc says: what is the reasoning of this? why would one want to do this?}

\cite{He:2015aa} compared the performance of various network configurations on ImageNet validation dataset. From these comparisons, we have selected the 34-layer network and the 54-layer network. The 34-layer network is consisting of pairs of $3 \times 3$ blocks. The 50-layer network is consisting of bottleneck blocks. In table \ref{tab:bottleneck-comparison} we have compared these networks by their structure, required number of FLOPs, and their top-1 and top-5 errors on this dataset. As we can see in the FLOPs, the networks have about $5\%$ of difference in number of floating point operations. As \cite{He:2015aa} reports, this small increase in parameters is effecting accuracy of the model considerably. 

\begin{table}[]
\centering
\begin{tabular}{ | c | c | c | c | }
\hline
layer name			& output size 					& 34-layer																& 50-layer																			\\ \hline
conv1				& $112 \times112$				& \multicolumn{2}{c|}{$ 7 \times 7$, $64$, stride $2$}																												\\ \hline
\multirow{2}{*}{conv2\_x}	& \multirow{2}{*}{$56 \times 56$} 	& \multicolumn{2}{c|}{$3 \times 3$ max pool, stride $2$}																											\\ \cline{3-4} 
					&							& $\begin{bmatrix} 3 \times 3, &   64 \\ 3 \times 3, &   64 \end{bmatrix} \times 3 $		& $\begin{bmatrix}1 \times 1, & 64 \\ 3 \times 3, & 64 \\ 1 \times 1, & 256 \end{bmatrix}^{} \times 3 $ 		\\ \hline
conv3\_x				& $28 \times 28$				& $\begin{bmatrix} 3 \times 3, & 128 \\ 3 \times 3, & 128 \end{bmatrix} \times 3 $		& $\begin{bmatrix}1 \times 1, & 128 \\ 3 \times 3, & 128 \\ 1 \times 1, & 512 \end{bmatrix} \times 3$		\\ \hline
conv4\_x				& $14 \times 14$				& $\begin{bmatrix} 3 \times 3, & 256 \\ 3 \times 3, & 256 \end{bmatrix} \times 3 $		& $\begin{bmatrix}1 \times 1, & 256 \\ 3 \times 3, & 256 \\ 1 \times 1, & 1024 \end{bmatrix} \times 3$		\\ \hline
conv5\_x				& $  7 \times   7$				& $\begin{bmatrix} 3 \times 3, & 512 \\ 3 \times 3, & 512 \end{bmatrix} \times 3 $		& $\begin{bmatrix}1 \times 1, & 512 \\ 3 \times 3, & 512 \\ 1 \times 1, & 2048 \end{bmatrix} \times 3$		\\ \hline
					& $  1 \times   1$				&\multicolumn{2}{c|}{average pool, 1000-d fc, softmax}																											\\ \hline
\multicolumn{2}{| c |}{FLOPs}							& $3.6 \times 10^9$														& $3.8 \times 10^9$																	\\ \hline
\multicolumn{2}{| c |}{top-1 error ($\%$)}						& $21.53$																& $20.74$																			\\ \hline
\multicolumn{2}{| c |}{top-5 error ($\%$)}						& $5.60$																& $5.25$																			\\ \hline
\multicolumn{2}{| c |}{top-1 error \small{($\%$, \textbf{10-crop} testing)}}						& $24.19$																& $22.85$																			\\ \hline
\multicolumn{2}{| c |}{top-5 error \small{($\%$, \textbf{10-crop} testing)}}						& $7.40$																& $6.71$																			\\ \hline
\end{tabular}
\caption{Comparison of bottleneck blocks (50-layer) with stacked $ 3 \times 3$ layers (34-layer). }
\label{tab:bottleneck-comparison}
\end{table}

But the main contribution of \cite{He:2015aa} is not the bottleneck architecture, but Residual Connections that we will see in another section. 


\section{Improving Network Efficiency}
\subsection{Residual Connections}
\cite{He:2015aa} is also introducing a method called Residual Connections.
\todoin{go into details of how this works using information given in \cite{He:2015aa} and \cite{DBLP:journals/corr/SzegedyIV16}}
\subsection{Batch Normalization}

















