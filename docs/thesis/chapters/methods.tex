% !TEX root = ../thesis.tex
So far we have explained some neural network building blocks and some techniques for reduced complexity or increased efficiency. In this chapter, first we are going to explain the experiments we ran to get a better understanding of these methods. Then, we are going to talk about the model we came up with combining these methods. 

\section{Pruning}
Since pruning individual weights do not affect complexity directly, we are going to focus on pruning nodes. To do that, as \cite{Hu:2016aa} explained, we are going to use \textit{training cycles}. We will define two neural networks for two easy problems. Then we will train each neural network using its training dataset. After the training is done, we will collect $\RELU$ output statistics on that training dataset without updating weights. Using these statistics, we will try to understand which nodes have minimal effect on the outcome. We will prune these nodes by removing the relevant dimensions from the weight matrices. Then we will go back to the training step. We will keep iterating over these steps until we can not find any nodes to be pruned. 

First we will explain these two easy problems. Then, we will define the methods that we have used to optimize the number of pruned nodes.


\subsection{Fully Connected Networks}
We have defined a neural network consisting of 2 input dimensions ($\xn{n} \inreal{2}$), one fully connected layer with 1000 nodes and one fully connected output with a single node ($\yn{n} \in \realR$). We have defined the expected output as the summation of two inputs, ($\yn = \xn{n,1} + \xn{n,2}$). As a result, we precisely know the optimum neural network structure that would be able to perform this calculation. Which is a neural network with one fully connected layer with one node and an output layer with one node fully connected to that. All weights equal to $1$ and all biases equal to $0$.

We have calculated the loss using $\RMSE$, and used Momentum Optimizer (learning rate $0.01$ and momentum $0.9$) to learn the weights. We have generated $1.000.000$ samples, we trained the network with batch size $1000$.

\subsection{Convolutional Neural Networks}
We have extended our pruning experiments to convolutional neural networks. To do so, we have trained an autoencoder on MNIST dataset. We have defined our autoencoder with two encoder and two decoder layers. Each encoder layer ($\convk{1}$ and $\convk{2}$) is running a convolution with kernel size $3$ and stride of $2$. After each encoding layer, we add bias, apply batch normalization and $\RELU$ activation. Each decoding layer ($\lftk{Deconv}{3}$ and $\lftk{Deconv}{4}$) is running deconvolutions with kernel size of 3 and strides of two. Followed by adding bias, batch normalization and $\RELU$ activations. We defined the loss as the root mean square of the input and the output of the network. 

Overall, the functions and the variables we have used to define this neural network are
\begin{equation*}
\begin{split}
\xn{n} \in \ &\real{28 \times 28 \times 1}\\
\convk{1}:& \real{28 \times 28 \times 1} \rightarrow \real{14 \times 14 \times 32} \\
\convk{2}:& \real{14 \times 14 \times 32} \rightarrow \real{7 \times 7 \times 64} \\
\lftk{Deconv}{3}:& \real{7 \times 7 \times 64} \rightarrow \real{14 \times 14 \times 32} \\
\lftk{Deconv}{4}:& \real{14 \times 14 \times 32} \rightarrow \real{28 \times 28 \times 1} \\
\loss =& \RMSE(\xn{n}, \yhn{n})
\end{split}
\end{equation*}

\subsection{Pruning Strategies}
In this subsection we will look at the strategies we have followed to optimize the number of pruned nodes.
\subsubsection{Activations}
We experiment with 2 pruning configurations. There are different alternatives on choosing the neurons or weighs to prune. But we stick to the two simplest methods that were explained.

\subsubsection{Regularization}
In both experiments, we test no regularization, L1 regularization and L2 regularization to see how they effect pruning.

\subsubsection{Distortion}
In cases where some nodes are mostly activated together, we can assume that they are representing similar features. To prevent such cases, we tried to distort the weights between training cycles and force some difference in feature representations.

\iffalse
\subsubsection{Activation Correlation}
We also collect statistics about activation correlations to see which nodes are activated together most of the times. Using this information we determine which nodes represent similar features. 
\todoin{i have removed that because we have not tested it properly}
\fi

\section{Convolution Operation Alternatives}
\label{sec:conv_alternatives}
In general, convolution operations are expensive. In this section, we experiment with \textit{kernel composing convolutions} and \textit{separable convolutions} to see which one is the better alternative. To see the differences between these operations, we ran two experiments for each. One classifying MNIST dataset, the other classifying CIFAR-10 dataset.

\subsection{Baseline Model}
We have defined a convolutional neural network with three convolutional layers followed by one fully connected layer. Each convolutional layer has kernel size 5. Convolutional layers are followed bias addition, batch normalization, and finally $\RELU$ activations. First two convolution layers are followed by a max pooling layer with kernel size $2$ and strides of $2$. The third convolutional layer is followed by a global average pooling layer, where we reduce the width and height dimensions to the average of all values in those dimensions. Using $\SCE$ loss and momentum optimizer (learning rate $10^{-4}$, and momentum $0.9$), we train these networks for $20000$ steps with batch size $32$. 

Overall, the functions and the variables we have used to define this neural network are
\begin{equation*}
\begin{split}
\xn{n} \in \ &\real{32 \times 32 \times 1}\\
\convk{1}:& \real{32 \times 32 \times 3} \rightarrow \real{32 \times 32 \times 32} \\
\avgpoolk{2}:& \real{32 \times 32 \times 32} \rightarrow \real{16 \times 16 \times 32} \\
\convk{3}:& \real{16 \times 16 \times 32} \rightarrow \real{16 \times 16 \times 64} \\
\avgpoolk{4}:& \real{16 \times 16 \times 64} \rightarrow \real{8 \times 8 \times 64} \\
\convk{5}:& \real{8 \times 8 \times 64} \rightarrow \real{8 \times 8 \times 128} \\
\avgpoolk{6}:& \real{8 \times 8 \times 128} \rightarrow \real{128} \\
\FCk{7}:& \real{128} \rightarrow \real{10} \\
\loss =& \SCE(\xn{n}, \yhn{n})
\end{split}
\end{equation*}


 $\lfkt{k}{KCConv}$ and $\lfkt{k}{SConv}$ we obtain 3 versions of this model. 
\subsection{Kernel Composing Convolutions}
By changing second and third $\convk{k}$ with $\lfkt{k}{KCConv}$ we compare the effects of using kernel composing convolution operation. Remember that kernel composing convolution operation requires an additional parameter for number of intermediate output channels. We use the number of output channels for that. 

\subsection{Separable Convolutions}
By changing second and third $\convk{k}$ with $\lfkt{k}{SConv}$ we compare the effects of using separable convolution operation.

\subsection{Non-Linear Separable Convolutions}
\cite{howard2017mobilenets} proposed that, adding batch normalization and $\RELU$ activations between depthwise and pointwise convolutions in separable convolutions would increase the model accuracy. To test that we compare non-linear separable convolutions and separable convolutions.

\begin{figure}
\vspace{-65px}
  \begin{center}
        \includegraphics{images/separable_resnet.eps}
  \end{center}
  \caption{Separable Resnet-34. Branching lines represent residual connections, dashed ones are padded with zeros to match the number of channels. If s=2, depthwise convolution is ran with strides of two and residual branch is average pooled with strides 2 and kernel size 2. $\RELU$ and Batch Normalization operations are hidden.}
  \label{fig:model}
\end{figure}

\section{Separable Resnet}
Inspired by ResNet (\cite{He:2015aa}, \cite{he2016identity}), we have created a convolutional neural network with residual connections. In Figure \ref{fig:model} we show the architecture of our neural network.


\subsection{Model Choices}
These are the choices that we made while designing our neural network. We try to keep the model complexity low while trying to achieve maximum accuracy.
\begin{figure}
\vspace{-65px}
\begin{center}
\includegraphics[width=0.38\textwidth]{images/full_preactivation.eps}
\end{center}
\caption{Full pre-activation residual connections.}
\label{fig:full-preactivation}
\end{figure}

\begin{itemize}
\item In \cite{He:2015aa}, ResNet-34 starts with a $7 \times 7$ convolution with strides of two and it is followed by max pooling layer with strides of two and kernel size 2. If we think about this design choice, we see that the kernel size choice ($7 \times 7$) is to minimize the loss of information caused by two layers with strides of two. From another point of view, those two layers decrease the image size from $224 \times 224$ to $56 \times 56$. This decreases the complexity of future layers. When we apply such a convolution, the first convolutional layer becomes very complex compared to the rest of the network. To prevent that we propose a different first layer. We propose to change the first convolutional kernels from $7 \times 7$ to $3 \times 3$ and halve the output channels. Then, before applying the max pooling layer, we apply a $3 \times 3$ depthwise convolution that multiplies the number of channels with two. Then we apply a $1 \times 1$ convolution (pointwise) to that.  By doing so we reduce the complexity of these layers about four times. We ran experiments comparing this proposed first layer and the original one.
\item Except for the first convolutional operation, we have replaced every convolution layer with a separable convolution layer. The first layer has 3 input and 32 output channels. Therefore, for this layer the number of FLOPs for a convolution operation ($\kernelsize*\kernelsize*\mk{k-1}*\mk{k} = 3*3*3*16 = 864$) is sufficiently higher than a separable convolution ($\kernelsize*\kernelsize*\mk{k-1} + \mk{k-1}\mk{k} = 3*3*3 + 3*32=123$). But for this layer, separable convolution comes with a disadvantage. By definition, it applies a depthwise convolution for color channels separately. We believe that the feature representations of the input are dependent on the information from different color channels. Therefore we argue that applying a depthwise convolution without mixing the colors is inefficient, and not change the first convolution operation to a separable convolution.
\item As \cite{he2016identity} proposed, we are using full pre-activation residual connections. See Figure \ref{fig:full-preactivation}.
\end{itemize}

\subsection{Training}
We train our network using CIFAR-10 and ImageNet datasets. 

\subsubsection{CIFAR-10}
We divided the dataset for 50.000 training images and 10.000 validation images. We used momentum optimizer with momentum $0.9$ and learning rates $0.1$, $0.01$, $0.001$ for steps $0$ to $40.000$, $40.000$ to $60.000$ and $60.000$ to $80.000$ respectively. We have defined the loss with $\SCE$ of the truth and prediction, with an addition of L2 norm of weights multiplied by $0.001$. We trained our model using the training images for $80.000$ steps with batch size $128$. 

We preprocess the images using the routines defined in Tensorflow tutorials\footnote{\url{https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10}}\footnote{\url{https://www.tensorflow.org/tutorials/deep\_cnn\#cifar-10\_model}}. We start by taking $24 \times 24$ random crops and then we randomly flip the image to left or right. Then we randomly change the brightness and contrast. Then we normalize this image by subtracting the mean and dividing by variance by using a method called per\_image\_standardize.

For CIFAR-10 training, we make some changes in our model. Since we have defined our input as a $24 \times 24$ image, we can apply a total of three convolutions with stride of two. After these, the image dimensions become $3 \times 3$. After this point for our convolutions to make sense, we can not apply convolutions with strides of two until we apply global average pooling. Therefore, we remove the strides from green and pink blocks in Figure \ref{fig:model} and we do not multiply the number of channels by two. 

We train our model with CIFAR-10 to be able to verify our configuration before we train on ImageNet. 

\subsubsection{ImageNet}
We trained our model in ImageNet training dataset. We used momentum optimizer with momentum $0.9$ and learning rates $0.01$, $0.001$, $0.0001$, $0.00001$ for steps $0$ to $150.000$, $150.000$ to $300.000$, $300.000$ to $500.000$ and $500.000$ to $600.000$ respectively. We have defined the loss with $\SCE$ of the truth and prediction, with an addition of L2 norm of weights multiplied by $0.001$. We train our model using the training images for $200.000$ steps with batch size $128$.

We preprocess images using the routines defined for open sourced Tensorflow implementation of inception network\footnote{\url{https://github.com/tensorflow/models/blob/master/slim/preprocessing/inception_preprocessing.py}}. We start by creating a new random bounding box overlapping with the original bounding box and make sure that 0.1 of the bounding box is inside our new bounding box. Then we crop this new bounding box and resize it using bilinear resizing algorithm. Then we randomly flip it to left or right. Then we distort the colors using random brightness and saturation. Then we normalize this input to the range of $[-1,1]$ by subtracting 0.5 and multiplying by 2. 

\subsection{Pruning Nodes}
After training the model for the given number of steps, we prune the nodes and retrain the model until it converges. We repeat this process until we can not find any nodes to prune.

\subsubsection{Pruning Residual Connections}
The addition operation in the residual block creates a one-to-one relationship between the outputs of different blocks. With the existence of such a relationship, it is not possible to prune a residual block's output nodes without pruning the former or latter blocks. Therefore, we first group the residual blocks that are directly connected to each other. We consider the residual blocks that do not apply pooling/padding operation to be directly connected to the previous residual block (in Figure \ref{fig:model}, consecutive straight arrows between dashed arrows are directly connected). Then we calculate the indexes of nodes to keep from the output of every residual block. We union these indexes and prune the remaining nodes from the outputs of every directly connected residual block. We also prune the operations within residual blocks separately. 

\subsection{Factorization}
We apply pruning weights, weight clustering before factorization to be able to reduce the computational complexity as much as possible.

\subsubsection{Pruning Weights}
We set weight values that are very close to $0$ to $0$. Despite the fact that this operation does not change computational complexity, when combined with factorization, it helps finding a lower rank.

\subsubsection{Weight Clustering}
We round the weights after the second decimal. Again, doing so helps factorization to have a lower rank. 

\subsubsection{Factorization}
We factorize the trained, pruned and rounded weights using SVD. We can not apply this method to depthwise convolution operation. So we only factorize the convolution, pointwise convolution and fully connected weights. To do that, we calculate $U$, $S$ and $V$ for each of the weight matrices. We lower the rank of the decomposition by one. Then, we calculate the approximation error. If the approximation error is above a threshold, we check if this low rank decomposition would reduce the complexity. If it does, we use the decomposition. Otherwise we use the non-factorized weights. 

\subsection{Quantization}
We quantize the weights from 32-bits to 8-bit and run performance and accuracy benchmarks.

\section{Benchmarking}
We benchmark our models on a \textit{One Plus X}\footnote{\url{https://oneplus.net/x/specs}} mobile device, equipped with a \textit{Snapdragon 801}\footnote{\url{https://www.qualcomm.com/products/snapdragon/processors/801}} chipset. Our benchmarks consist of running consecutive inferences on a model, for a period of time. While those inferences are running, we collect hardware statistics using simpleperf\footnote{\url{https://android.googlesource.com/platform/prebuilts/simpleperf/}}. Simpleperf lets us collect the hardware level performance statistics for a given process.

Our benchmarking app uses a static input image (with dimensions depending on the model). So that we can ignore the overhead of pre-processing. Also, we perform no post-processing. Doing so, we try to avoid the effects of any other computation that could change the benchmark results. 

We ran this benchmarking tool for various models. Since running the benchmark does not require a trained network, we could easily generate multiple models and benchmark them. These models include; Inception-Resnet v2 (\cite{DBLP:journals/corr/SzegedyIV16}), Inception v1 (\cite{Szegedy:2014aa}), Inception v2 (\cite{Szegedy:2014aa}), Inception v3 (\cite{Szegedy_2016_CVPR}), Inception v4 (\cite{DBLP:journals/corr/SzegedyIV16}), VGG-19 (\cite{Simonyan:2014aa}), ResNet-50, ResNet-101, ResNet-152 and ResNet-200 (\cite{He:2015aa}, \cite{he2016identity}) and Mobilenet (\cite{howard2017mobilenets}).  


\iffalse

\begin{verbatim}
Performance counter statistics:

     2,589,237,398  L1-dcache-loads           # 940.825 M/sec       (14%)
                 0  L1-dcache-load-misses     # 0.000 /sec          (100%)
     3,393,233,002  L1-dcache-stores          # 980.415 M/sec       (17%)
                 0  L1-dcache-store-misses    # 0.000 /sec          (100%)
     2,404,509,748  L1-icache-loads           # 576.568 M/sec       (21%)
                 0  L1-icache-load-misses     # 0.000 /sec          (100%)
     2,919,824,326  L1-icache-stores          # 597.244 M/sec       (24%)
                 0  L1-icache-store-misses    # 0.000 /sec          (100%)
     5,798,305,546  dTLB-loads                # 1.023 G/sec         (28%)
     5,037,941,181  dTLB-stores               # 1.024 G/sec         (25%)
        68,593,055  iTLB-loads                # 16.289 M/sec        (21%)
        56,479,236  iTLB-stores               # 16.014 M/sec        (18%)
       337,848,194  branch-loads              # 121.453 M/sec       (14%)
                 0  branch-load-misses        # 0.000 /sec          (100%)
       408,888,494  branch-stores             # 115.616 M/sec       (18%)
                 0  branch-store-misses       # 0.000 /sec          (100%)
                 0  node-loads                # 0.000 /sec          (21%)
                 0  node-load-misses          # 0.000 /sec          (100%)
                 0  node-stores               # 0.000 /sec          (25%)
                 0  node-store-misses         # 0.000 /sec          (100%)
                 0  node-prefetches           # 0.000 /sec          (28%)
                 0  node-prefetch-misses      # 0.000 /sec          (100%)
    26,196,665,969  cpu-cycles                # 3.754399 GHz        (35%)
    18,807,785,910  instructions              # 3.014 G/sec         (31%)
       506,029,677  branch-instructions       # 91.660 M/sec        (28%)
        13,950,967  branch-misses             # 2.897 M/sec         (24%)
    15,055,849,062  bus-cycles                # 3.623 G/sec         (21%)
                 0  stalled-cycles-frontend   # 0.000 /sec          (14%)
                 0  stalled-cycles-backend    # 0.000 /sec          (14%)
  47497.934289(ms)  cpu-clock                 #                     (100%)
  47497.483193(ms)  task-clock                # 2.373245 cpus used  (100%)
         1,889,335  page-faults               # 94.405 K/sec        (100%)
            30,549  context-switches          # 1.526 K/sec         (100%)
             1,481  cpu-migrations            # 74.002 /sec         (100%)
         1,889,294  minor-faults              # 94.403 K/sec        (100%)
                 6  major-faults              # 0.300 /sec          (100%)
                 0  alignment-faults          # 0.000 /sec          (100%)
                 0  emulation-faults          # 0.000 /sec          (100%)
\end{verbatim}
\fi

