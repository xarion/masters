% !TEX root = ../thesis.tex
So far we have explained some neural network building blocks and some techniques for reduced complexity. We have run some experiments to better understand some of these techniques. In this chapter, we are going to talk about these experiments and tools we had to built to run these experiments. First we identified some methods and to understand them better, we ran experiments on them. Then we have tried to combine these methods to come up with a model. 

\section{Pruning}
Since pruning individual weights do not effect complexity directly, we are going to focus on pruning nodes. To do that, as \cite{Hu:2016aa} explained, we are going to use \textit{training cycles}. First, we will define a neural network and a problem. Then we will train this network using the training dataset. After the training is done, we will run inference on the training dataset and collect ReLU output statistics. Using these statistics, we will try to understand which nodes have minimal effect on the outcome. We will prune these nodes by removing the relevant dimensions from the weight matrices. Then we will go back to the training step. We will keep iterating over these steps until we can't find any nodes to be pruned.

\subsection{Fully Connected Summation}
To increase our understanding on this method, we started with a very easy problem. We have implemented a neural network consisting of 2 input dimensions ($\xn{n} \inreal{2}$), one fully connected layer with 1000 nodes and a one fully connected output with a single node ($\yn{n} \in \realR$). We have defined the expected output as the summation of two inputs, ($\yn = \xn{n,1} + \xn{n,2}$). So that we precisely know the optimum neural network structure that would be able to perform this calculation. Which is a neural network with one fully connected layer with one node and an output layer with one node fully connected to that. All weights equal to $1$ and all biases equal to $0$.

We have calculated the loss using mean squared error, and used Momentum Optimizer (learning rate $0.01$ and momentum $0.9$) to learn the weights. We have generated $1.000.000$ samples, we trained the network with batch size $1000$.

\subsection{MNIST Autoencoder}
To expand our pruning experiments to convolutional neural networks, we have implemented an autoencoder for MNIST Dataset. An autoencoder consists of two parts. First is the encoder, which aims to reduce the dimensionality of input. The other is decoder, which aims to convert the encoded data back to it's original form. Therefore an autoencoder reduces the dimensions of an input data and then tries to recreate that data from those reduced dimensions.

We have defined the auto encoder with two encoder layers followed by two decoder layers. Each encoding layer running a convolution with kernel size $3$ and stride of $2$. Following each of these, we are applying batch normalization and ReLU activation. Each decoding layer is running deconvolutions with kernel size $3$ and stride of $2$. Followed by adding bias, batch normalization and ReLU activations. We defined the loss as the root mean square of the input and the output of the network. 

The information contained in one input image $\xn{n} \inreal{28 \times 28 \times 1}$ is represented with $784$ floating points. Therefore, a good auto-encoder should be reducing this number with each encoding layer. While doing so, it should be able to convert the encoded image back to it's original form with minimal loss while decoding. 

Since we will try to find and prune unused nodes, we will start with a configuration that is larger than necessary. 
\begin{equation*}
\begin{split}
\xn{n} &\inreal{28 \times 28 \times 1}\\
\convk{1}:& \real{28 \times 28 \times 1} \rightarrow \real{14 \times 14 \times 32} \\
\convk{2}:& \real{14 \times 14 \times 32} \rightarrow \real{7 \times 7 \times 64} \\
\lftk{Deconv}{3}:& \real{7 \times 7 \times 64} \rightarrow \real{14 \times 14 \times 32} \\
\lftk{Deconv}{4}:& \real{14 \times 14 \times 32} \rightarrow \real{28 \times 28 \times 1} \\
\loss =& RMSE(\xn{n}, \yhn{n})
\end{split}
\end{equation*}

\subsection{Regularization}
In both experiments, we test no regularization, L1 regularization and L2 regularization to see how they effect the activation statistics and pruning.

\subsection{Distortion}
In case of two nodes in one layer, if the weights connecting to them are proportional, it may not be possible to prune one of them. To prevent that, we introduce a little bit distortion to weights, forcing nodes representing similar features to have a difference. 

\subsection{Activations}
We test 2 pruning configurations for activation counts, activation values. There are many alternatives on choosing the neurons or weighs to prune. But we stick to the two simplest methods.

\subsubsection{Activation Counts}
We count the activations per node, regardless the activation value. Using this information we determine which nodes are not used. We set a range using the mean and variance of activation counts. We prune the nodes outside this range.

\subsubsection{Activation Values}
We collect statistics about output values per node. Using this information we determine which nodes are more important for the results by calculating the variance per node and removing the low variance ones regardless of the mean value.

\iffalse
\subsubsection{Activation Correlation}
We also collect statistics about activation correlations to see which nodes are activated together most of the times. Using this information we determine which nodes represent similar features. 
\todoin{i have removed that because we haven't tested it properly}
\fi

\section{Convolution Operation Alternatives}
In general, convolution operations are expensive. In this section, we experiment with \textit{kernel composition} and \textit{separable convolutions} to see which one is the better alternative to regular convolutions. To see the differences between both operations, we ran two experiments for each. One classifying MNIST dataset, the other classifying CIFAR-10 dataset.

We have defined a network with three convolutional layers followed by one fully connected layer. Each convolutional layer has kernel size 5. Convolutional layers are followed by batch normalization, then ReLU activations. First two convolution layers are followed by an average pooling layer with kernel size $2$ and strides of $2$. The third convolutional layer is followed by a global average pooling layer, where we reduce the width and height dimensions to the average of all values in those dimensions. We using SCE loss and momentum optimizer (learning rate $10^{-4}$, and momentum $0.9$), we train this network for $20000$ steps with batch size $32$. 

\begin{equation*}
\begin{split}
\xn{n} &\inreal{32 \times 32 \times 1}\\
\convk{1}:& \real{32 \times 32 \times 3} \rightarrow \real{32 \times 32 \times 32} \\
\avgpoolk{2}:& \real{32 \times 32 \times 32} \rightarrow \real{16 \times 16 \times 32} \\
\convk{3}:& \real{16 \times 16 \times 32} \rightarrow \real{16 \times 16 \times 64} \\
\avgpoolk{4}:& \real{16 \times 16 \times 64} \rightarrow \real{8 \times 8 \times 64} \\
\convk{5}:& \real{8 \times 8 \times 64} \rightarrow \real{8 \times 8 \times 128} \\
\avgpoolk{6}:& \real{8 \times 8 \times 128} \rightarrow \real{128} \\
\FCk{7}:& \real{128} \rightarrow \real{10} \\
\loss =& SCE(\xn{n}, \yhn{n})
\end{split}
\end{equation*}


By changing $\convk{k}$ with $\lfkt{k}{ConvCompose}$ and $\lfkt{k}{Separable}$ we obtain 3 versions of this model. Please remember that $\lfkt{k}{ConvCompose}$ requires an additional parameter for number of intermediate output channels. We use the number of output channels for that. We train each configuration for both MNIST and CIFAR-10 for a fair comparison.

\begin{figure}
\vspace{-65px}
  \begin{center}
        \includegraphics{images/separable_resnet.eps}
  \end{center}
  \caption{Separable Resnet-34. Branching lines represent residual connections, dashed ones are padded with zeros to match the number of channels. If s=2, depthwise convolution is ran with strides of two and residual branch is average pooled with strides 2 and kernel size 2. ReLU and Batch Normalization operations are hidden.}
  \label{fig:model}
\end{figure}

\subsection{Non-Linearity in Separable Convolutions}
\cite{howard2017mobilenets} proposed that, adding batch normalization and ReLU activations between depthwise and pointwise convolutions would increase the model accuracy. To test that we ran the previous experiment comparing non-linear separable convolutions and separable convolutions.



\section{Separable Resnet}
Inspired by ResNet (\cite{He:2015aa}, \cite{he2016identity}), we have created a convolutional neural network with residual connections. In Figure \ref{fig:model} we show the architecture of our neural network. 


\subsection{Model Choices}
These are the choices that we made while designing our neural network. We try to keep the model complexity low while trying to achieve maximum accuracy.
\begin{figure}
\vspace{-65px}
\begin{center}
\includegraphics[width=0.38\textwidth]{images/full_preactivation.eps}
\end{center}
\caption{Full pre-activation residual connections.}
\label{fig:full-preactivation}
\end{figure}

\begin{itemize}
\item In \cite{He:2015aa}, ResNet-34 starts with a $7 \times 7$ convolution with strides of two and it is followed by max-pooling layer with strides of two and kernel size 2. If we think about this design choice, we see that the kernel size choice ($7 \times 7$) is to minimize the loss of information caused by two layers with strides of two. From another point of view, those two layers decrease the image size from $224 \times 224$ to $56 \times 56$. This decreases the complexity of future layers. When we apply such a convolution, the first convolutional layer becomes very complex compared to the rest of the network. To prevent that we propose a different first layer. We propose to change the first convolutional kernels from $7 \times 7$ to $3 \times 3$ and halve the output channels. Then, before applying the max pooling layer, we apply a $3 \times 3$ dephtwise convolution that multiplies the number of channels with two. Then we apply a $1 \times 1$ convolution (pointwise) to that.  By doing so we reduce the complexity of these layers about four times. We ran experiments comparing this proposed first layer and the original one.
\item Except for the first convolutional operation, we have replaced every convolution layer with a separable convolution layer. The first layer has 3 input and 32 output channels. Therefore, for this layer the number of FLOPs for a convolution operation ($\kernelsize*\kernelsize*\mk{k-1}*\mk{k} = 3*3*3*16 = 864$) is sufficiently higher than a separable convolution ($\kernelsize*\kernelsize*\mk{k-1} + \mk{k-1}\mk{k} = 3*3*3 + 3*32=123$). But for this layer, separable convolution comes with a disadvantage. By definition, it applies a depthwise convolution for color channels separately. We believe that the feature representations of the input are dependent on the information from different color channels. Therefore we argue that applying a depthwise convolution without mixing the colors is inefficient, and not change the first convolution operation to a separable convolution.
\item As \cite{he2016identity} proposed, we are using full pre-activation residual connections. See Figure \ref{fig:full-preactivation}.
\end{itemize}

\subsection{Training}
We train our network using CIFAR-10 and ImageNet datasets. 

\subsubsection{CIFAR-10}
We divided the dataset for 50.000 training images and 10.000 validation images. We used momentum optimizer with momentum $0.9$ and learning rates $0.1$, $0.01$, $0.001$ for steps $0$ to $40.000$, $40.000$ to $60.000$ and $60.000$ to $80.000$ respectively. We have defined the loss with SCE of the truth and prediction, with an addition of L2 norm of weights multiplied by $0.001$. We trained our model using the training images for $80.000$ steps with batch size $128$. 

We preprocess the images using the routines defined in tensorflow tutorials\footnote{\url{https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10}}\footnote{\url{https://www.tensorflow.org/tutorials/deep\_cnn\#cifar-10\_model}}. We start by taking $24 \times 24$ random crops and then we randomly flip the image to left or right. Then we randomly change the brightness and contrast. Then we normalize this image by subtracting the mean and dividing by variance by using a method called per\_image\_standardize.

For CIFAR-10 training, we make some changes in our model. Since we have defined our input as a $24 \times 24$ image, we can apply a total of three convolutions with stride of two. After these, the image dimensions become $3 \times 3$. After this point for our convolutions to make sense, we can't apply convolutions with strides of two until we apply global average pooling. Therefore, we remove the strides from green and pink blocks in Figure \ref{fig:model} and we don't multiply the number of channels by two. 

We train our model with CIFAR-10 to be able to verify our configuration before we train on ImageNet. 

\subsubsection{ImageNet}
We trained our model in ImageNet training dataset. We used momentum optimizer with momentum $0.9$ and learning rates $0.01$, $0.001$, $0.0001$, $0.00001$ for steps $0$ to $150.000$, $150.000$ to $300.000$, $300.000$ to $500.000$ and $500.000$ to $600.000$ respectively. We have defined the loss with SCE of the truth and prediction, with an addition of L2 norm of weights multiplied by $0.001$. We train our model using the training images for $200.000$ steps with batch size $128$.

We preprocess images using the routines defined for open sourced tensorflow implementation of inception network\footnote{\url{https://github.com/tensorflow/models/blob/master/slim/preprocessing/inception_preprocessing.py}}. We start by creating a new random bounding box overlapping with the original bounding box and make sure that 0.1 of the bounding box is inside our new bounding box. Then we crop this new bounding box and resize it using bilinear resizing algorithm. Then we randomly flip it to left or right. Then we distort the colors using random brightness and saturation. Then we normalize this input to the range of $[-1,1]$ by subtracting 0.5 and multiplying by 2. 

\subsection{Pruning Nodes}
\subsubsection{Pruning Residual Connections}
The addition operation in the residual block creates a one-to-one relationship between the outputs of different blocks. With the existence of such a relationship, it is not possible to prune a residual block's output nodes without pruning the former or latter blocks. Therefore we first group the residual blocks that are directly connected to each other. We consider the residual blocks that don't apply pooling/padding operation to be directly connected to the previous residual block (in Figure \ref{fig:model}, consecutive straight arrows between dashed arrows are directly connected). Then we calculate the indexes of nodes to keep from the output of every residual block. We union these indexes and prune the remaining nodes from the outputs of every directly connected residual block. We also prune the operations within residual blocks separately. 

\subsubsection{Pruning with Adam Optimizer}
We have trained our network using Adam Optimizer (\cite{kingma2014adam}) as well. Adam optimizer keeps two moment matrices for each weight matrix. Therefore, if we are pruning a node, we prune relevant moment matrices accordingly.

\subsection{Factorization}
\subsubsection{Pruning Weights}
We set weight values that are very close to $0$ to $0$. Despite the fact that this operation does not change computational complexity, when combined with factorization, it helps achieving a lower rank decomposition.

\subsubsection{Rounding Weights}
We round the weights after the second decimal. Doing so helps factorization to have a lower rank. 

\subsubsection{Factorization}
We factorize the trained, pruned and rounded weights using SVD. We can not apply this method to depthwise convolution operation. So we only factorize the convolution, pointwise convolution and fully connected weights. To do that, we calculate $U$, $S$ and $V$ for each of the weight matrices. We lower the rank of the decomposition by one. Then we calculate the approximation error. If the approximation error is above a threshold, we check if this low rank decomposition would reduce the complexity. If it does, we use the decomposition. Otherwise we use the non-factorized weights. 

\subsection{Quantization}
We quantize the weights from 32-bits to 8-bit and run performance and accuracy benchmarks.

\section{Benchmarking}
We benchmark our models on a \textit{One Plus X}\footnote{\url{https://oneplus.net/x/specs}} mobile device, equipped with a \textit{Snapdragon 801}\footnote{\url{https://www.qualcomm.com/products/snapdragon/processors/801}} chipset. Our benchmarks consist of running consecutive inferences on a model, for a period of time. While those inferences are running, we collect hardware statistics using simpleperf\footnote{\url{https://android.googlesource.com/platform/prebuilts/simpleperf/}}. Simpleperf lets us collect the hardware level performance statistics for a given process.

Our benchmarking app uses a static input image (with dimensions depending on the model). So that we can ignore the overhead of pre-processing. Also, we perform no post-processing. Doing so, we try to avoid the effects of any other computation that could change the benchmark results. 

We ran this benchmarking tool for various models. Since running the benchmark doesn't require a trained network, we could easily generate multiple models and benchmark them. These models include; Inception-Resnet v2 (\cite{DBLP:journals/corr/SzegedyIV16}), Inception v1 (\cite{Szegedy:2014aa}), Inception v2 (\cite{Szegedy:2014aa}), Inception v3 (\cite{Szegedy_2016_CVPR}), Inception v4 (\cite{DBLP:journals/corr/SzegedyIV16}), VGG-19 (\cite{Simonyan:2014aa}), ResNet-50, ResNet-101, ResNet-152 and ResNet-200 (\cite{He:2015aa}, \cite{he2016identity}) and Mobilenet (\cite{howard2017mobilenets}).  


\iffalse

\begin{verbatim}
Performance counter statistics:

     2,589,237,398  L1-dcache-loads           # 940.825 M/sec       (14%)
                 0  L1-dcache-load-misses     # 0.000 /sec          (100%)
     3,393,233,002  L1-dcache-stores          # 980.415 M/sec       (17%)
                 0  L1-dcache-store-misses    # 0.000 /sec          (100%)
     2,404,509,748  L1-icache-loads           # 576.568 M/sec       (21%)
                 0  L1-icache-load-misses     # 0.000 /sec          (100%)
     2,919,824,326  L1-icache-stores          # 597.244 M/sec       (24%)
                 0  L1-icache-store-misses    # 0.000 /sec          (100%)
     5,798,305,546  dTLB-loads                # 1.023 G/sec         (28%)
     5,037,941,181  dTLB-stores               # 1.024 G/sec         (25%)
        68,593,055  iTLB-loads                # 16.289 M/sec        (21%)
        56,479,236  iTLB-stores               # 16.014 M/sec        (18%)
       337,848,194  branch-loads              # 121.453 M/sec       (14%)
                 0  branch-load-misses        # 0.000 /sec          (100%)
       408,888,494  branch-stores             # 115.616 M/sec       (18%)
                 0  branch-store-misses       # 0.000 /sec          (100%)
                 0  node-loads                # 0.000 /sec          (21%)
                 0  node-load-misses          # 0.000 /sec          (100%)
                 0  node-stores               # 0.000 /sec          (25%)
                 0  node-store-misses         # 0.000 /sec          (100%)
                 0  node-prefetches           # 0.000 /sec          (28%)
                 0  node-prefetch-misses      # 0.000 /sec          (100%)
    26,196,665,969  cpu-cycles                # 3.754399 GHz        (35%)
    18,807,785,910  instructions              # 3.014 G/sec         (31%)
       506,029,677  branch-instructions       # 91.660 M/sec        (28%)
        13,950,967  branch-misses             # 2.897 M/sec         (24%)
    15,055,849,062  bus-cycles                # 3.623 G/sec         (21%)
                 0  stalled-cycles-frontend   # 0.000 /sec          (14%)
                 0  stalled-cycles-backend    # 0.000 /sec          (14%)
  47497.934289(ms)  cpu-clock                 #                     (100%)
  47497.483193(ms)  task-clock                # 2.373245 cpus used  (100%)
         1,889,335  page-faults               # 94.405 K/sec        (100%)
            30,549  context-switches          # 1.526 K/sec         (100%)
             1,481  cpu-migrations            # 74.002 /sec         (100%)
         1,889,294  minor-faults              # 94.403 K/sec        (100%)
                 6  major-faults              # 0.300 /sec          (100%)
                 0  alignment-faults          # 0.000 /sec          (100%)
                 0  emulation-faults          # 0.000 /sec          (100%)
\end{verbatim}
\fi

