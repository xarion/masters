% !TEX root = ../thesis.tex
\iffalse
Taken from http://www.ldeo.columbia.edu/~martins/sen_sem/thesis_org.html
Methods
What belongs in the "methods" section of a scientific paper?
    Information to allow the reader to assess the believability of your results.
    Information needed by another researcher to replicate your experiment.
    Description of your materials, procedure, theory.
    Calculations, technique, procedure, equipment, and calibration plots. 
    Limitations, assumptions, and range of validity.
    Desciption of your analystical methods, including reference to any specialized statistical software. 
The methods section should answering the following questions and caveats: 
    Could one accurately replicate the study (for example, all of the optional and adjustable parameters on any sensors or instruments that were used to acquire the data)?
    Could another researcher accurately find and reoccupy the sampling stations or track lines?
    Is there enough information provided about any instruments used so that a functionally equivalent instrument could be used to repeat the experiment?
    If the data are in the public domain, could another researcher lay his or her hands on the identical data set?
    Could one replicate any laboratory analyses that were used? 
    Could one replicate any statistical analyses?
    Could another researcher approximately replicate the key algorithms of any computer software?
Citations in this section should be limited to data sources and references of where to find more complete descriptions of procedures.
Do not include descriptions of results. 
\fi
\section{Dependencies}
\subsection{Tensorflow}
In our research, we will strictly use Tensorflow \cite{abadi2016tensorflow}. \todo[inline]{What is tensorflow, why we chose it, what are the advantages of using it, what are the limitations that come with it}
\todo[inline]{things to be explained: FC layer, Convolution Operation, what do we mean when we say neuron/node, activation, bias, training dataset}
\section{Pruning}
Pruning aims to reduce the number of operations by deleting the parameters that has low or no impact in the result. Studies show that applying this method in an ANN is effective in reducing the model complexity, improving generalization, and they are effective in reducing the required training cycles. In our experiments we will try to reproduce these effects.
To visualize these methods, let's think of two fully connected layers, $\mathbf{l_1}$ and $\mathbf{l_2}$. $\mathbf{l_1}$ is the input of this operation and it consists of $N$ values, $\mathbf{l_1}=(l_{11}, l_{12}, ..., l_{1N})$. $\mathbf{l_2}$ is the output of this operation consists of $M$ values, $\mathbf{l_2}=(l_{21}, l_{22}, ..., l_{2M})$. Between these two layers, there is a weight matrix $W$ with size $N \times M$. The operation, that we want to optimize is, $\mathbf{l_2} = \mathbf{l_1}W$.
\todo[inline]{maybe explain in more detail and give examples of pruning algorithms here. (e.g. Optimal Brain Damage, Second order derivatives for network pruning: Optimal Brain Surgeon, Optimal Brain Surgeon and general network pruning, SEE Pruning Algorithms-a survey from R. Reed)}

\subsection{Pruning Individual Weights}
With this subcategory of pruning algorithms we want to optimize the number of floating point operations by removing some values from $W$. Theoretically, it makes sense to remove individual scalars from W, and exclude operations related to them. This would ideally reduce the required number of floating point operations. But in our library of our choice, Tensorflow, matrix multiplication implementation \texttt{tf.matmul} do not consider such a change. It takes two fixed size matrices, and does the computations using all of their values. Tensorflow also has another matrix multiplication operation, \texttt{tf.sparse\_tensor\_dense\_matmul}. This operation takes a sparse matrix and a dense matrix as inputs and outputs a dense matrix. To implement this method, we could convert W to a sparse tensor after pruning the weights. But, Tensorflow documentations about this method state;
\begin{itemize}
\item Will the SparseTensor A fit in memory if densified?
\item Is the column count of the product large ($>> 1$)?
\item Is the density of A larger than approximately $15\%$?
\end{itemize}
"If the answer to several of these questions is yes, consider converting the SparseTensor to a dense one.". In our terms, SparseTensor A is corresponding to the pruned version of $W$. 

Since $W$ was already dense before, we can assume that the answer to the first question is yes. The column count of our product is $M$ which is much larger than $1$ in some cases. Also we don't know anything about the density of pruned version of $W$. Looking at these facts, we are assuming that implementing this operation will be problematic. Instead of delving deeper into these problems to evaluate this method, we will move on to other methods.

\subsection{Activation Based Pruning - Fully Connected Layers} \label{sec:activation-based-pruning-convolution}
Activation based pruning, works by looking at individual values in layers, and prunes the layer and corresponding weight row/columns completely. To visualize this, we will assume that the fully connected layers we have defined are, trained to some extent, and activated using ReLU activations. With this definition, if we apply our dataset and count the number of activations in $\mathbf{l_1}$ and $\mathbf{l_2}$, we may realize that there are some neurons that are not being activated at all. By removing these neurons from the layers, we can reduce the number of operations. This removal operation is done by removing neurons based on their activations. 

\subsubsection{Experiment Set-up}
To verify the validity of this method, we set up a basic experiment. We have implemented a neural network consisting of 2 inputs, $\mathbf{i} = (i_1, i_2)$, 1 fully connected layer with $n = 1000$ hidden nodes and 1 output, $o$. We have used ReLU \cite{nair2010rectified} activations on our hidden layer. For the sake of simplicity, we have defined the expected output $y$ as $y = i_1 + i_2$. We chose a simple problem so that we precisely know the most optimum neural network structure that would be able to perform this calculation. Which is the same network where the fully connected layer has one hidden node, all weights equal to $1$ and all biases equal to $0$.

We have calculated the loss using mean squared error, and optimized it using Momentum Optimizer (learning rate $0.01$ and momentum $0.3$). Using $1.000.000$ samples, we trained the network with batch size $1000$. With these parameters, we ran a training session with 10 epochs and we have observed that the loss didn't converge to 0. Therefore, the model was unable to find the correct solution with this optimizer. 
\todo[inline]{We should also try different optimizers here}
\subsubsection{Vanilla Pruning}
First we have implemented the very basic idea of pruning unused activations. To do so, we defined training cycles based on the method defined in \cite{Hu:2016aa}. In each training cycle, 1) we have trained the model for some epochs, 2) we lock the weights, 3) feed the training data to the network and count the activations for each neuron in the hidden layer, 4) prune the neurons that have less than or equal to the activation threshold, 5) go back to step $1$ if some neurons were pruned, stop otherwise.

When tested with $0$ activation threshold, after the first training cycle, this method did not to prune more weights. In our experiments, we have pruned approximately 950 weights out of 1000. This result is promising but at the same time, it's not close enough to the result we were expecting. We delved deeper into the source of this issue.

\todo[inline]{We should try different optimizers and make the beginning of the case about why we decided to distort weights.Tell that we have checked the gradients and seen that they were mostly in one direction (+). }

\subsubsection{Distorted Pruning}
When we inspected the gradients of weights, we have seen that most of them were in the positive direction. In our case, this trend in gradients is not helping with the understanding of which neurons are necessary, and which are not. This trend can also be understood as, the feature representation is shared among different hidden neurons. 
\todo[inline]{talk about what does "all gradients are in the positive direction" mean for feature representation}
To prevent shared feature representation, we have decided to push the weights to different directions, randomly. This resulted with the result closes to optimum, but not the exact solution that we were looking for. 
\todo[inline]{the results were in a form not resembling the real solution. maybe because floating point numbers not adding up perfectly, but the result is almost the same in terms of our loss. the exact values of weights and biases are: \\
\texttt{w1: [[ 0.74285096], [ 0.64994317]]\\
b1: [ 7.80925274]\\
w2: [[-6.75151157]]\\
b2: [ 7.80925274]}\\
So since our random values are between -1 and 1, these values are actually okay.}
\todo[inline]{talk about how you decide on the amount of distortion (currently $rand(weights.shape) * (1-var(weights))$). Talk about what changed when we introduced }
\subsubsection{Regularized Distorted Pruning}
Since the solution we found is only resembling our result under some boundaries, we have decided to add an l1 regularizer to our loss. By doing so we are aiming to push the high bias and w2 values closer to 0. But it doesn't really make any difference when used with Moment Optimizer.


\subsection{Activation Based Pruning - Convolution and Deconvolutions}
\todo[inline]{Put references for conv and deconv operations. }
In theory, convolution operation is a matrix multiplication applied on a sliding window. Thus, counting the output feature activations of a convolution operation, we can apply activation based pruning. 

\subsubsection{Experiment Set-up}
To verify the validity of this method, we have implemented an auto encoder for MNIST Dataset \cite{lecun1998mnist}. MNIST contains $28 \times 28$ grayscale images of handwritten digits. The autoencoder consists of two parts. First part is the encoder. The encoder aims to reduce the dimensionality of input. The decoder aims to convert the encoded data back to it's original form.

We have defined the auto encoder with two encoder blocks followed by two decoder blocks. Each encoding block is running convolutions with kernel size $3$, strides of $2$ and $SAME$ padding. Then we are adding bias to this result, following this we are applying batch normalization \cite{ioffe2015batch} and then ReLU activation \cite{nair2010rectified}. Each decoding block is running deconvolutions with kernel size $3$ and strides of $2$. Followed by adding bias, batch normalization and ReLU activations. 

The information contained in one $28 \times 28 \times 1$ matrix is represented with $784$ units (floating points in this case). Therefore, a good autoencoder should be capable of reducing this number when encoding. Similarly, converting the reduced matrix back to it's original form with minimal loss while decoding. Also, the simplest autoencoder should be capable of keeping this number same among blocks while keeping the loss at 0. 

In our case, our encoder blocks are reducing the matrix width and height to half. Therefore, if they output $4$ times the number of input channels, they should represent the same information losslessly. Similarly our decoder blocks are doubling the matrix width and height. Therefore if they output a quarter of the number of input channels, they should be able to decode the encoded information perfectly. In Table \ref{tab:mnist_baseline_encoder} we have defined the layer output dimensions for that baseline auto-encoder.
\begin{table}
\begin{center}
\begin{tabular}{ c | c }
 Block Name & Output Dimensions ($h \times w \times c$) \\
 \hline
 Input Image & $28 \times 28 \times 1$ \\
 Encoder 1 & $14 \times 14 \times 4$ \\  
 Encoder 2 & $7 \times 7 \times 16$ \\
 Decoder 1 & $14 \times 14 \times 4$ \\  
 Decoder 2 & $28 \times 28 \times 1$ 
\end{tabular}
\end{center}
\caption{The baseline network that could perform lossless encoding in theory.}
\label{tab:mnist_baseline_encoder}
\end{table}

To define the network to experiment on, we chose $[32, 64, 32]$ as the output channels of Encoder 1, Encoder 2 and Decoder 1 respectively.

\todo[inline]{This definition may not be good. check it.}

\subsubsection{Vanilla Pruning}
As we did in the Fully Connected Layers, we have pruned the connections that are not being activated. In these experiments we have seen that the network has been pruned insignificantly. After applying this method, we have achieved a network consisting of $[16, 64, 22]$ output channels for blocks Encoder 1, Encoder 2 and Decoder 1 respectively.

\subsubsection{Distorted Pruning}
\todo[inline]{this doesn't change anything.}



\todo[inline]{we can also check activation probabilities and make a decisions based on this data}




\section{Efficient Structures}
\subsection{1D Convolutions}
Convolution kernels could be decomposed into smaller operations. For example using SVD, we can decompose the convolution kernel into the multiplication for two smaller kernels, and by applying these kernels consequently, we can approximate the convolution operation with fewer floating point operations. Instead of doing separating learned convolutions, we are going to use the method introduced in \cite{alvarez2016decomposeme}. This method forces the separability of convolution operation as a hard constraint. 

Normally convolution operations are defined 2 dimensional ($2D$). That is, kernel sizes are ($2D$). For example convolutions we have used in Section \ref{sec:activation-based-pruning-convolution} are $2D$, their kernel sizes are $3 \times 3$. With this method, we are aiming to construct a $N \times N$ convolution operation as a combination two convolution operations. The first convolution has kernel size $1 \times N$ and the following convolution has $N \times 1$, or vice versa. Looking back at the experiment we did in  Section \ref{sec:activation-based-pruning-convolution}, instead of applying one $3 \times 3$ convolution, we are talking about applying one $1 \times 3$ convolution followed by a $3 \times 1$ convolution. 

The amount of speed up can be calculated with Big-O notation($\mathcal{O}$). A convolution operation can be seen as a matrix multiplication applied to consequent subsections of an image. To visualize this, let's assume that we have a convolution operation with kernel size $N$, input channels $K$ and output channels $L$. Assuming that we are applying that operation to a $N \times N$ image patch with $K$ channels, our operation can be simplified as multiplying this $N*N*K$ vector with $N*N*K \times L$ matrix. The complexity of this operation can be expressed as $\mathcal{O}(N^2KL)$. When composed as two $1D$ compositions, this operation will be represented with two $1D$ convolutions. First convolution is multiplying vector $N*K$ with matrix $N*K \times P$ and the second convolution is multiplying vector $N*P$ with matrix $N*P \times L$. As you can see we have introduced the variable $P$ as the intermediate output of the first convolution. The complexity of these operations can be described as, $\mathcal{O}(NLP)$ and $\mathcal{O}(NPK)$. And summing them up, we can say $\mathcal{O}(NP(L + K))$. To determine if this decomposition is making things basic, we could basically try to define a meaningful $P$ while trying to make sure that $\frac{NP(L+K)}{N^2KL} < 1$.
\todo[inline]{shall we cite Big $\mathcal{O}$?}

To show the validity of this method, we have conducted two experiments. 
\subsubsection{Experiment - MNIST Classification}
Our first experiment is to apply this decomposition on a convolutional classifier for MNIST Dataset. This classifier is originally consisting of three Convolutional Layers and one Fully Connected layer. This configuration is defined in Table \ref{tab:nxn-mnist-classifier}. By converting the 
\begin{table}
\centering
\begin{tabular}{l | c | c}
Layer & Configuration & Output\\
\hline
Input Image & & $28 \times 28 \times 1$ \\
\hline
Convolution & \small $N=5, \text{strides}=1, \text{padding}=SAME$ & $28 \times 28 \times 32$ \\
Add Bias & & $28 \times 28 \times 32$ \\
ReLU & & $28 \times 28 \times 32$ \\
Max Pool & size=$ 2 \times 2$ & $14 \times 14 \times 32$ \\
\hline
Convolution & \small $N=5, \text{strides}=1, \text{padding}=SAME$ & $14 \times 14 \times 64$ \\
Add Bias & & $14 \times 14 \times 64$ \\
ReLU & & $14 \times 14 \times 64$ \\
Max Pool & size=$ 2 \times 2$ & $7 \times 7 \times 64$ \\
\hline
Convolution & \small $N=7, \text{strides}=1, \text{padding}=VALID$ & $1 \times 128$ \\
Add Bias & & $ 1 \times 128$ \\
ReLU & & $ 1 \times 128$ \\
\hline
FC Layer &  & $1 \times 10$ \\
Add Bias & & $1 \times 10$ 
\end{tabular}
\caption{Network configuration for MNIST Classifier, output of every row is applied as the input of next.}
\label{tab:nxn-mnist-classifier}
\end{table}
































