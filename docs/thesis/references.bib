%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Erdi Calli at 2017-04-04 10:25:01 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{alvarez2016decomposeme,
	Author = {Alvarez, Jose and Petersson, Lars},
	Date-Added = {2017-04-04 08:24:57 +0000},
	Date-Modified = {2017-04-04 08:24:57 +0000},
	Journal = {arXiv preprint arXiv:1606.05426},
	Title = {Decomposeme: Simplifying convnets for end-to-end learning},
	Year = {2016}}

@article{ioffe2015batch,
	Author = {Ioffe, Sergey and Szegedy, Christian},
	Date-Added = {2017-03-30 14:52:33 +0000},
	Date-Modified = {2017-03-30 14:52:33 +0000},
	Journal = {arXiv preprint arXiv:1502.03167},
	Title = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
	Year = {2015}}

@misc{lecun1998mnist,
	Author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher JC},
	Date-Added = {2017-03-30 14:28:25 +0000},
	Date-Modified = {2017-03-30 14:28:25 +0000},
	Title = {The MNIST database of handwritten digits},
	Year = {1998}}

@inproceedings{nair2010rectified,
	Author = {Nair, Vinod and Hinton, Geoffrey E},
	Booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
	Date-Added = {2017-03-29 14:17:21 +0000},
	Date-Modified = {2017-03-29 14:17:21 +0000},
	Pages = {807--814},
	Title = {Rectified linear units improve restricted boltzmann machines},
	Year = {2010}}

@article{abadi2016tensorflow,
	Author = {Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
	Date-Added = {2017-03-27 14:38:13 +0000},
	Date-Modified = {2017-03-27 14:38:13 +0000},
	Journal = {arXiv preprint arXiv:1603.04467},
	Title = {Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
	Year = {2016}}

@article{He:2015aa,
	Abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	Date-Added = {2017-03-16 15:46:41 +0000},
	Date-Modified = {2017-03-16 15:46:41 +0000},
	Eprint = {1512.03385},
	Month = {12},
	Title = {Deep Residual Learning for Image Recognition},
	Url = {https://arxiv.org/abs/1512.03385},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1512.03385}}

@article{Zagoruyko:2016aa,
	Abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at},
	Author = {Sergey Zagoruyko and Nikos Komodakis},
	Date-Added = {2017-03-16 15:45:59 +0000},
	Date-Modified = {2017-03-16 15:45:59 +0000},
	Eprint = {1605.07146},
	Month = {05},
	Title = {Wide Residual Networks},
	Url = {https://arxiv.org/abs/1605.07146},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1605.07146}}

@article{Simonyan:2014aa,
	Abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	Author = {Karen Simonyan and Andrew Zisserman},
	Date-Added = {2017-03-16 15:42:08 +0000},
	Date-Modified = {2017-03-16 15:42:08 +0000},
	Eprint = {1409.1556},
	Month = {09},
	Title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	Url = {https://arxiv.org/abs/1409.1556},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1409.1556}}

@article{Canziani:2016aa,
	Abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
	Author = {Alfredo Canziani and Adam Paszke and Eugenio Culurciello},
	Date-Added = {2017-03-16 15:35:12 +0000},
	Date-Modified = {2017-03-16 15:35:12 +0000},
	Eprint = {1605.07678},
	Month = {05},
	Title = {An Analysis of Deep Neural Network Models for Practical Applications},
	Url = {https://arxiv.org/abs/1605.07678},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1605.07678}}

@article{Szegedy:2014aa,
	Abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	Author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
	Date-Added = {2017-03-16 15:16:50 +0000},
	Date-Modified = {2017-03-16 15:16:50 +0000},
	Eprint = {1409.4842},
	Month = {09},
	Title = {Going Deeper with Convolutions},
	Url = {https://arxiv.org/abs/1409.4842},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1409.4842}}

@inproceedings{rust121997activity,
	Author = {Rust12, Alistair G and George, Stella and Bolouri23, Hamid},
	Booktitle = {Fourth European Conference on Artificial Life},
	Date-Added = {2017-03-16 14:03:26 +0000},
	Date-Modified = {2017-03-16 14:03:26 +0000},
	Organization = {MIT Press},
	Pages = {224},
	Title = {Activity-based pruning in developmental artificial neural networks},
	Year = {1997}}

@article{Babaeizadeh:2016aa,
	Abstract = {Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks and datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.},
	Author = {Mohammad Babaeizadeh and Paris Smaragdis and Roy H. Campbell},
	Date-Added = {2017-03-16 12:56:50 +0000},
	Date-Modified = {2017-03-16 12:56:50 +0000},
	Eprint = {1611.06211},
	Month = {11},
	Title = {NoiseOut: A Simple Way to Prune Neural Networks},
	Url = {https://arxiv.org/abs/1611.06211},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1611.06211}}

@article{Hu:2016aa,
	Abstract = {State-of-the-art neural networks are getting deeper and wider. While their performance increases with the increasing number of layers and neurons, it is crucial to design an efficient deep architecture in order to reduce computational and memory costs. Designing an efficient neural network, however, is labor intensive requiring many experiments, and fine-tunings. In this paper, we introduce network trimming which iteratively optimizes the network by pruning unimportant neurons based on analysis of their outputs on a large dataset. Our algorithm is inspired by an observation that the outputs of a significant portion of neurons in a large network are mostly zero, regardless of what inputs the network received. These zero activation neurons are redundant, and can be removed without affecting the overall accuracy of the network. After pruning the zero activation neurons, we retrain the network using the weights before pruning as initialization. We alternate the pruning and retraining to further reduce zero activations in a network. Our experiments on the LeNet and VGG-16 show that we can achieve high compression ratio of parameters without losing or even achieving higher accuracy than the original network.},
	Author = {Hengyuan Hu and Rui Peng and Yu-Wing Tai and Chi-Keung Tang},
	Date-Added = {2017-03-16 12:34:27 +0000},
	Date-Modified = {2017-03-16 12:34:27 +0000},
	Eprint = {1607.03250},
	Month = {07},
	Title = {Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures},
	Url = {https://arxiv.org/abs/1607.03250},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1607.03250}}
