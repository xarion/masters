\documentclass[a4paper,man,natbib]{apa6}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}

\title{Optimizing Neural Networks for Mobile Devices}
\shorttitle{Activation Based Pruning}
\author{Erdi \c{C}all{\i}}
\affiliation{Radboud University Nijmegen, Neurant}

\abstract{Recent developments in Neural Networks(or Deep Learning) are promising. Some models are capable of accomplishing tasks as good as humans, or better. But we still lack the applications that are available to the public. The general opinion is, Neural Network models need expensive equipment. But that only applies to the training process, where the network learns from data. But using the trained model for inference is easier. There are also methods to reduce the computational complexity of a model. With these methods, we may be able to use cheap compute devices(i.e. Mobile Phones) for inference. Thus making some models available to public. }



\begin{document}
\maketitle
\section{Introduction}
Recent state of the art Deep Learning models are surpassing previous methods. Fields such as; computer vision, automatic speech recognition, natural language processing, speech recognition, and bioinformatics make use of these models. They use deep models consisting of many layers (e.g. 152 layers in  \cite{He:2015aa}), many parameters in each layer, and as a result, a lot of Floating Point Operations to run Inference (e.g. $11.3 \times 10^9$ in  \cite{He:2015aa}).
In contrast, mobile devices have limited processing power and memory. Also the best practice is to provide a fluent user experience with low response time. Thus, we should change these models to provide a good user experience.
There is research on methods to define optimized models or optimize a given model. These methods consist; pruning unimportant parameters, using less bits to represent parameters, or using less parameters by using more optimized structures. 

In this research we are going run experiments to answer;
\begin{enumerate}
\item    Which models are running slow in Mobile Devices?
\item    Why these models are running slow?
\item    Which methods can we use to optimize these models?
\item    What is the trade off of using these methods?
\item    Why an optimization technique is working or not on a model?
\item    Can we define a more optimized model for the same task?
\item    How can we combine different optimization techniques?
\item    Are these optimized models efficient enough to run in Mobile Devices? 
\end{enumerate}

\section{Recent Studies}
Artificial Neural Networks (ANN) have several parameters such as number of hidden layers, number of neurons in a layer, or the structure of a layer. Until now, we have seen different combinations for these parameters. For example, \cite{Simonyan:2014aa} introduces a model called VGGNet. VGGNet introduces more layers (16 to 19) than the previous models. They show how this parameter effects the accuracy. \cite{He:2015aa} introduces the residual connections. This new connection between layers is capable of stacking more layers than before. Training up to 152 layers, they show superior accuracy. \cite{Zagoruyko:2016aa} compares having higher number of neurons in each layer to having more layers. Each combination resulting in a unique model with a different accuracy level. In contrast to all these, \cite{Szegedy:2014aa} suggests something different. Having a good harmony within the network works better than having more parameters. Supporting that, \cite{Canziani:2016aa} does a detailed comparison of different models. They show that, increasing the number of hidden layers or the number of neurons in a layer does not necessarily increase the accuracy. 

Following these, we think that, some models are over-parameterized. Meaning they contain parameters that they are not making use of. Therefore, they are making unnecessary computations with them.


\section{Methods}
\subsection{Activation Based Pruning}

\subsubsection{Introduction}

Studies show that pruning unnecessary neurons in an ANN is effective in reducing the model complexity. Activation based pruning, sums up the neuron activation counts. Using that information, it determines the useless neurons, and deletes them. In theory, this method also reduces the training time per cycle, and prioritizes important connections. We set up an experiment to see the efficiency of this method. We defined a problem with a simple optimal solution, summation of two inputs. We defined a network with redundant parameters and trained it in different configurations. Our experiments show that this method is working to its expectations. \todo[inline]{We can talk 
about how it is important in the nature, check \cite{rust121997activity}}

\bibliography{references}

\end{document}
