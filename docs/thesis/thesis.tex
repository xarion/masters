\documentclass[12pt]{report}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{array,booktabs}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{csquotes}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{wrapfig}
\usepackage{mathtools}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{pdfpages}
\usepackage{subcaption}

\newcommand{\todoin}{\todo[inline]}
\newcommand{\realR}{\mathbb{R}}
\newcommand{\real}[1]{\realR^{#1}}
\newcommand{\inreal}[1]{\in \real{#1}}

\newcommand{\weights}{w}
\newcommand{\w}{\weights}
\newcommand{\wk}[1]{\weights^{(#1)}}
\newcommand{\wki}[2]{\wk{#1}_{#2}}

\newcommand{\all}[1]{\expandafter\MakeUppercase\expandafter{#1}}
\newcommand{\num}[1]{\expandafter\MakeUppercase\expandafter{#1}}

\newcommand{\layer}{l}
\newcommand{\lk}[1]{\layer^{(#1)}}
\newcommand{\lki}[2]{\layer^{(#1)}_{#2}}

\newcommand{\numnodes}{m}
\newcommand{\m}{\numnodes}
\newcommand{\mk}[1]{\numnodes^{(#1)}}
\newcommand{\mki}[2]{\numnodes^{(#1)}_{#2}}

\newcommand{\outputvar}{o}
\newcommand{\ok}[1]{\outputvar^{(#1)}}
\newcommand{\oki}[2]{\outputvar^{(#1)}_{#2}}
\newcommand{\okT}[1]{\outputvar^{(#1)T}}
\newcommand{\okiT}[2]{\outputvar^{(#1)T}_{#2}}


\newcommand{\biasterm}{b}
\newcommand{\bk}[1]{\biasterm^{(#1)}}
\newcommand{\bki}[2]{\biasterm^{(#1)}_{#2}}

\newcommand{\layerfunction}{\psi}

\newcommand{\lf}{\layerfunction}
\newcommand{\lfp}{\lf'}
\newcommand{\lft}[1]{\lf^{(#1)}}
\newcommand{\lfk}[1]{\lf_{(#1)}}
\newcommand{\lftk}[2]{\lf^{(#1)}_{(#2)}}
\newcommand{\lfkt}[2]{\lf_{(#1)}^{(#2)}}
\newcommand{\lfpk}[1]{\lf'_{(#1)}}
\newcommand{\lfpkt}[2]{\lf_{(#1)}'^{(#2)}}

\newcommand{\FC}{\lft{FC}}
\newcommand{\FCk}[1]{\FC_{(#1)}}

\newcommand{\lftp}[1]{\lf'^{(#1)}}
\newcommand{\FCp}{\lftp{FC}}
\newcommand{\FCkp}[1]{\FCp_{(#1)}}

\newcommand{\conv}{\lft{Conv}}
\newcommand{\convk}[1]{\conv_{(#1)}}

\newcommand{\convp}{\lftp{Conv}}
\newcommand{\convkp}[1]{\convp_{(#1)}}

\newcommand{\maxpool}{\lft{maxpool}}
\newcommand{\maxpoolk}[1]{\maxpool_{(#1)}}

\newcommand{\avgpool}{\lft{avgpool}}
\newcommand{\avgpoolk}[1]{\avgpool_{(#1)}}

\newcommand{\activationfunction}{\sigma}
\newcommand{\act}{\activationfunction}

\newcommand{\datain}{x}
\newcommand{\x}{\datain}
\newcommand{\xn}[1]{\datain_{#1}}
\newcommand{\xni}[2]{\datain_{#1,#2}}

\newcommand{\datatruth}{y}
\newcommand{\y}{\datatruth}
\newcommand{\yn}[1]{\datatruth_{#1}}
\newcommand{\yni}[2]{\datatruth_{#1,#2}}

\newcommand{\approximations}{\hat{\datatruth}}
\newcommand{\yh}{\approximations}
\newcommand{\yhn}[1]{\approximations_{#1}}
\newcommand{\yhni}[2]{\approximations_{#1,#2}}

\newcommand{\bigo}[1]{\mathcal{O}(#1)}

\newcommand{\nnfunc}{f}
\newcommand{\loss}{\mathcal{L}}

\newcommand{\width}{\mathcal{W}}
\newcommand{\widthk}[1]{\mathcal{W}_{#1}}

\newcommand{\height}{\mathcal{H}}
\newcommand{\heightk}[1]{\mathcal{H}_{#1}}

\newcommand{\patch}{p}
\newcommand{\p}{\patch}
\newcommand{\pk}[1]{\patch^{(#1)}}
\newcommand{\pki}[2]{\patch^{(#1)}_{#2}}

\newcommand{\kernelsize}{K}

\newcommand{\stride}{s}
\newcommand{\s}{\stride}
\newcommand{\sk}[1]{\s_{#1}}

\newcommand{\imagedimsk}[1]{\real{\heightk{#1} \times \widthk{#1} \times \mk{#1} }}

\newcommand{\RELU}{\textrm{ReLU}}
\newcommand{\RMSE}{\textrm{RMSE}}
\newcommand{\CE}{\textrm{CE}}
\newcommand{\SCE}{\textrm{SCE}}

\newcommand*{\thead}[1]{\multicolumn{1}{c}{\bfseries #1}}

\title{
{\textbf{Master's Thesis}\\Faster Convolutional Neural Networks}\\
{\large Radboud University, Nijmegen}
}
\author{Erdi \c{C}all{\i}}

\begin{document}
\maketitle
\chapter*{Abstract}
There exists a gap between the computational cost of state of the art image processing models and the processing power of publicly available devices. This gap is reducing the applicability of these promising models. Trying to bridge this gap, first we investigate some methods to reduce the computational cost of a model. Secondly, we look for alternative operations to design state of the art models. Using these alternative operations, we train a model for the CIFAR-10 classification task. Our model achieves similar results ($91.1\%$ top-1 accuracy) to ResNet-20 ($91.25\%$ top-1 accuracy) with 2 times smaller the model size and 3 times fewer floating point operations. We observe that it is not possible to reduce the computational cost of our model using techniques such as pruning and factorization.
%\chapter*{Dedication}
%To mum and dad

%\chapter*{Declaration}
%I declare that..

%\chapter*{Acknowledgements}
%I want to thank...

\tableofcontents

\chapter{Introduction}
\input{chapters/introduction}

\chapter{Methods}
\input{chapters/methods}

\chapter{Results}
\input{chapters/results}

\chapter{Discussion}
\input{chapters/discussion}

\chapter{Conclusion}
\input{chapters/conclusion}

\bibliographystyle{alpha}
\bibliography{references}

\end{document}
